\section{Regresyon, Sınıflandırma ve Kümeleme Hata Metrikleri}

\subsection{Sınıflandırma Metrikleri}

\textbf{Karmaşıklık Matrisi (Confusion Matrix):} Sınıflandırma performasını ölçmek için kullanılır. Sütunlar gerçek verileri, satırlar ise tahmin edilen verileri gösterir. \\

\textbf{True Positive (TP):} Hasta olduğunu düşündünüz ve gerçekten hastasınız. True Negative (TN): Hasta olmadığını düşündünüz ve hasta değilsiniz. False Positive (FP): Hasta olduğunu düşündünüz fakat hasta değilsiniz. False Negative (FN): Hasta olmadığınızı düşündünüz fakat hastasınız. \\

\textbf{False Positive Rate (FPR):} 

\[\text{False Positive Rate (FPR)} = \frac{\text{False Positives}}{\text{False Positives} + \text{True Negatives}}\]

\textbf{True Positive Rate (TPR):}

\[\text{True Positive Rate (TPR)} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}\]

\textbf{Duyarlılık Skoru (Recall Score):} Duyarlılık, modelinizin pozitif olan verilerin ne kadarını pozitif olarak tahmin ettiğini gösterir. FN tahminlemenin maliyetinin yüksek olduğu durumlarda önemlidir. Duyarlılık yüksekse, model daha fazla gerçek pozitifi tespit edebiliyor demektir.

\[\text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}\]

\textbf{Kesinlik Skoru (Precision / Sensitivity Score):} Kesinlik, modelizin pozitif olarak tahmin ettiği verilerin gerçekte ne kadarının pozitif olduğudur. FP tahminlemenin maliyetinin yüksek olduğu durumlarda önemlidir. Precision ile Recall ters orantılıdır ve ikisinin arasında bir denge sağlamak gerekir (precision recall trade-off). Kesinlik yüksekse, modelin pozitif tahminleri genellikle doğru demektir.

\[\text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}\]

\textbf{Precision-Recall Trade-off:} Modelin kesinlik ve duyarlılık arasındaki dengeyi nasıl ayarladığını gösterir. İdeal bir model, hem yüksek kesinlik hem de yüksek duyarlılık sağlar. Birçok sınıflandırma algoritması, bu dengeyi kurmak için eşik değeri sunar. Eşik değeri değiştikçe, modelin kesinlik ve duyarlılık değerleri değişir. Örneğin eşik değeri yükseltildiğinde kesinlik artar ancak doğruluk düşer. Tam tersi, eşik değeri düşürüldüğünde duyarlılık artar ancak kesinlik düşer. Bir uygulamada, kesinlik ve duyarlılık arasındaki tradeof, problem bağlamına ve hedeflere bağlı olarak ayarlanır. Örneğin, tıbbi bir teşhis uygulamasında daha yüksek bir duyarlılık kritik olabilirken, spam e-posta filtrelemesi gibi bir uygulamada daha yüksek kesinlik öncelikli olabilir.

\textbf{Doğruluk Skoru (Accuracy Score):} Doğruluk, modelinizin doğru tahmin ettiği verilerin toplam veri kümesine oranını gösterir.

\[\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}\]

\textbf{F1 Skoru (F1 Score):} F1, kesinlik ve duyarlılık skorlarının harmonik ortalamasını gösterir. Harmonik olmasının sebebi; eğer basit bir ortalama hesaplansaydı Precision Değeri 1 ve Recall Değeri 0 olan bir modelin skoru 0.5 olacaktır ve bu bizi yanıltacaktır. Eşit dağılmayan veri kümelerinde (imbalanced) hatalı bir model seçimi yapmamak için accuracy yerine f1 değerine bakılır.

\[\text{F1} = 2 * \frac{\text{Precision} * \text{Recall}}{\text{Precision} + \text{Recall}}\]

\textbf{F2 Skoru (F2 Score):} F2, hassasiyetin (precision) önemini azaltır ve duyarlılığın (recall) önemini artırır. Yanlış negatifleri (FN) en aza indirmeye odaklanır.

\[\text{F2} = \frac{\text{TP}}{\text{TP} + 0.2 * \text{FP} + 0.8 * \text{FN}}\]

\textbf{F3 Soru (F3 Score):}

\[\text{F3} = \frac{\text{TP}}{\text{TP} + 0.1 * \text{FP} + 0.9 * \text{FN}}\]

\textbf{F-beta}: Beta parametresi ile precision ve recall arasındaki denge ağırlıklandırılır. Beta 0 iken yalnızca kesinliği (recall) dikkate alınır, +inf iken yalnızca hassasiyet (precision) dikkate alınır.

\[\text{F-}\beta = \frac{1 + \beta^2}{\frac{\beta^2}{\text{Recall}} + \frac{1}{\text{Precision}}}\]

\textbf{ROC-AUC Eğrisi (ROC-AUC Curve):} ROC modelin TPR ile FPR cinsinden ne kadar iyi ayrım yapabildiğini açıklar. AUC, ROC eğrisinin altında kalan alanı verir, 0-1 arasındadır. 0'ise bütün tahminler yanlıştır. 1'e yaklaştıkça modelin performansı iyileşir. ROC-AUC 0.5 ise, modelin performansı rastgele tahmin etmekle (yazı-tura tahmini \%50-\%50) aynıdır. \\

\textbf{Balanced Accuracy Score):} Dengesiz veri kümelerinde kullanılır. Her sınıfın doğruluk oranını dengeler ve ardından tüm sınıfların doğruluk oranlarının ortalamasını alır. Her bir sınıfın veri dağılımındaki etkisini eşit şekilde değerlendirir.

\[\text{Balanced Accuracy} = \frac{1}{\text{N}} * \sum{\frac{\text{TP}}{\text{P}}}\]

\textbf{Top K Accuracy Score:} Modelin en yüksek olasılığı sahip olan k tahmininin doğru sınıfı içerip içermediğini kontrol eder. Genelde çoklu sınıflı sınıflandırma problemlerinde kullanılır.

\[\text{Top K Accuracy Score} = \frac{\text{Number of correct predictions in top K}}{\text{Total number of samples}}\]

\textbf{Average Precision Score:} Dengesiz veri kümelerinde ve precision-recall analizlerinde kullanılır. Modelin gerçek pozitiflerini doğru bir şekilde saptama yeteneği ve yanlış pozitifler oluşturma konusundaki hassasiyetini gösterir. Precision-Recall eğrisi altında kalan alanı ifade eder. Precision-Recall eğrisi farklı kesme noktalarındaki precision ve recall değerlerini gösterir. Average precision, bu eğrinin altındaki alanın ortalamasını alarak hesaplanır. 1'e ne kadar yakınsa model o kadar iyi performans göstermiştir.

\[\text{Average Precision Score} = \frac{1}{n} \sum_{k=1}^{n} P(k) \times \Delta r(k)\]

\textbf{Log Loss:} Sınıflandırma modellerinin tahminlerinin olasılık dağılımıyla uyumunu ölçen bir metriktir. Gerçek etiketlerin olasılık dağılımı ve modelin ürettiği olasılık dağılımı arasındaki farkı ölçerek modelin kalitesini değerlendirir. Ne kadar düşükse o kadar iyidir. Modelin tahminleri gerçek etiketlere daha yakınsa, log\_loss değeri daha düşük olur. Model ne kadar belirsiz tahminler yapıyorsa log\_loss o kadar yüksek olur.\\

\textbf{Jaccard Score:} İkili sınıflandırma problemleri için kullanılır. Modelin gerçek pozitif ve negatif sınıfları ne kadar iyi tahmin ettiğini ölçer. Gerçek ve tahmin edilen sınıfların kesişimini gerçek ve tahmin edilen sınıfların birleşimine böler. 1'e ne kadar yakınsa o kadar iyidir.

\[\text{Jaccard Score} = \frac{\text{TP}}{\text{TP + FP + FN}}\]

\textbf{Spherical Payoff}: İki vektör arasındaki açısal farkı ölçerek, bu vektörlerin birbirine ne kadar yakın olduğunu değerlendirir. Bu metrik, genellikle doğruluk veya benzerlik ölçüsü olarak kullanılır ve bu sayede modelin performansını değerlendirir. İki vektör arasındaki kosinüs benzerliği hesaplanır. Değer 1'e yaklaştıkça tahmin edilen vektör ile gerçek vektör arasında yüksek bir benzerlik vardır.

\[ \text{Spherical Payoff} = \frac{\mathbf{y} \cdot \mathbf{\hat{y}}}{\|\mathbf{y}\| \|\mathbf{\hat{y}}\|} \]

\subsection{Regresyon Metrikleri}

\textbf{AIC (Akaike Information Criterion)}: bir modelin uyum kalitesini ve karmaşıklığının dikkate alarak model seçimi yapar. AIC değeri ne kadar düşükse model o kadar iyidir.

\[ \text{AIC} = 2k - 2\ln(L) \]

\textbf{BIC (Bayesian Information Criterion)}: modelin uyumunu ve karmaşıklığını değerlendirir. Model karmaşıklığını cezalandırmada daha güçlüdür. BIC değeri ne kadar düşükse model o kadar iyidir.

\[ \text{BIC} = k \ln(n) - 2\ln(L) \]

\textbf{R-Squared (R-Kare)}: R2, bağımlı değişkenin varyansının bağımsız değişkenler tarafından ne kadar açıklandığını gösterir. 0 ile 1 arasında değer alır. En iyi R2 değeri 1'dir. Daha yüksek R2 değerleri modelin veriyi daha iyi açıkladığını gösterir. R2 eksi değer alırsa, modelin underfit edildiğini gösterir. Multivariate Regression'da R2 yerine adjusted-r2 değerine bakılır.

\[R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}\]

\textbf{Mean Squared Error (MSE):} MSE, bir regresyon modelinin tahminlerinin gerçek değerlerden ne kadar sapma gösterdiğini ölçer. Her bir gözlem için hata karelerinin ortalaması alınır. Daha yüksek MSE değerleri, daha büyük hata anlamına gelir. MSE değeri ne kadar düşükse, modelin tahminlerinin gerçek değerlere o kadar yakın olduğunu gösterir. MSE sıfıra yaklaştıkça model daha iyi performans gösterir.

\[\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2\]

\textbf{Mean Absolute Error (MAE):} MAE, bir regresyon modelinin tahminlerinin gerçek değerlerden ortalama mutlak sapma gösterdiğini ölçer. Her bir gözlem için mutlak hataların ortalaması alınır. Düşük MAE değeri, modelin tahminlerinin gerçek değerlere yakın olduğunu gösterir. MAE sıfıra yaklaştıkça model daha iyi performans gösterir.

\[\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|\]

\textbf{Root Mean Squared Error (RMSE):} RMSE, MSE'nin kareköküdür. MSE'den farkı, hataların karelerinin büyüklüklerini daha iyi ifade etmesidir. RMSE değeri ne kadar düşükse, modelin tahminlerinin gerçek değerlere o kadar yakın olduğunu gösterir. RMSE sıfıra yaklaştıkça model daha iyi performans gösterir.

\[\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}\]

\textbf{Mean Percentage Error (MPE):} MPE, tahmin hatalarının gerçek değerlere göre yüzde olarak ne kadar sapma gösterdiğini ölçer. Pozitif ve negatif hataların etkisi birbirini sıfırlayabilir. MPE değeri ne kadar düşükse, modelin tahminlerinin gerçek değerlere ne kadar yakın olduğunu gösterir. MPE sıfıra yaklaştıkça model daha iyi performans gösterir.

\[\text{Mean Percentage Error (MPE)} = \frac{1}{n} \sum_{i=1}^{n} \left( \frac{y_i - \hat{y}_i}{y_i} \right) \times 100\%\]

\textbf{Explained Variance Score:} Modelin verideki değişkenliğin ne kadarını açıkladığını ölçer. 1' ne kadar yakınsa o kadar iyidir. Eğer 0 ise model verideki değişkenliği açıklayamıyor demektir. Eğer negatif ise model basit bir ortalama kullanmaktan daha kötü performans gösteriyor demektir. Modelin gerçek değerler üzerinde ne kadar başarılı tahminler yaptığını ve bu tahminlerin gerçek verideki değişkenliği ne kadar açıkladığını anlamak için kullanılır.

\[\text{Explained Variance Score} = 1 - \frac{\text{Var}(y - \hat{y})}{\text{Var}(y)}\]

\textbf{Max Error:} Modelin gerçek ve tahmin edilen değerler arasındaki en büyük farkı ölçer. Gerçek ve tahmin edilen değerler arasındaki maksimum mutlak farkı ifade eder.

\[\text{Explained Variance Score} = 1 - \frac{\text{Var}(y - \hat{y})}{\text{Var}(y)}\]

\textbf{Winkler Interval Score:} Modelin tahminlerinin gerçek değer dağılımına ne kadar yakın olduğunu gösterir. Modelin ürettiği tahminler gerçek değerler dağılımına göre bir aralık ile sınırlandırılır. Aralıktaki tahminlerin oranı hesaplanır. 0-1 arasında değer alır. 1'e ne kadar yakınsa o kadar iyidir.

\[\text{Winkler Interval Score} = \alpha \times (d - \Delta) \quad \text{if } d \leq \Delta, \quad \text{else} \quad \beta \times (d - \Delta) + \Delta\]

\subsection{Kümeleme Metrikleri}

\textbf{Mutual Info Score:} Gerçek sınıf metrikleri ile kümele sonuçları arasındaki ilişkiyi ölçer. Yüksek MIS değeri, kümeleme sonuçlarının gerçek sınıf etiketleriyle güçlü bir şekilde ilişkili olduğunu ve algoritmanın veri yapısını doğru bir şekilde kavradığını gösterir. Düşük MIS değeri, kümeleme sonuçlarının gerçek sınıf etiketlerine göre zayıf bir ilişki içinde olduğunu ve algoritmanın iyileştirilmesi gerektiğini gösterir.

\[\text{Mutual Info Score} = \frac{I(X;Y)}{\sqrt{H(X) \cdot H(Y)}}\]

\textbf{Rand Score:}

\[\text{Rand Index} = \frac{a + b}{\binom{n}{2}}\]

\textbf{Completeness Score:}

\[\text{Completeness Score} = \frac{1}{n} \sum_{i=1}^{n} \max_j (\text{Homogeneity}_{ij})\]

\textbf{Homogenity Score:}

\[\text{Homogeneity Score} = 1 - \frac{H(C|K)}{H(C)}\]

\textbf{V-Measure Score:}

\[\text{V-Measure Score} = \frac{2 \times \text{Homogeneity} \times \text{Completeness}}{\text{Homogeneity} + \text{Completeness}}\]

\newpage