\section{Explanabile AI}
Explainable AI (XAI), modellerin içsel işleyişlerini, karar süreçlerini ve sonuçlarını anlamayı ve açıklamayı sağlayan bir yaklaşımı ifade eder. XAI, yapay zeka sistemlerinin kararlarının nedenleri, faktörleri ve sonuçları hakkında şeffaf ve anlaşılabilir bir şekilde bilgi sunarak kullanıcıların güvenini ve kabul edilebilirliğini artırmayı amaçlar. XAI'nin birkaç ana hedefi vardır:
\begin{itemize}
    \item \textbf{Şeffaflık ve Anlaşılırlık:} Yapay zeka modellerinin içsel işleyişlerinin ve karar süreçlerinin anlaşılabilir ve şeffaf olmasını sağlamak.
    \item \textbf{Güvenirlik ve Kabul Edilebilirlik:} Kullanıcıların yapay zeka sistemlerine olan güvenini artırmak ve kararlarının kabul edilebilirliğini sağlamak.
    \item \textbf{Hata Tespiti ve Düzeltme:} Yanıltıcı veya hatalı sonuçları tespit ederek ve düzelterek yapay zeka modellerinin performansını iyileştirmek.
\end{itemize}

İki türde açıklama metotları vardır.
\begin{itemize}
    \item \textbf{Transparent Methods (Şeffaf Yöntemler):} Modelin doğrudan içsel yapısını ve karar süreçlerini açıklar. Modelin nasıl çalıştığını ve belirli bir tahminin nasıl yapıldığını açıklamak için kullanılır. Genellikle karar ağaçları, lineer regresyon gibi basit ve yorumlanabilir modelleri içerir.
    \item \textbf{Post-hoc Methods (Sonradan Uygulanan Yöntemler):} Modelin tahminlerini ve kararlarını daha sonra açıklamak için kullanılan dışsal yöntemlerdir. Modelin karmaşıklığını azaltmak ve anlaşılabilir bir şekilde sunmak için kullanılır. SHAP, LIME gibi yöntemler uygulanır.
\end{itemize}

\subsection{Local Interpretable Model Agnostic Explanations (LIME)}
LIME (Local Interpretable Model Agnostic Explanations), bir modelin belirli bir tahmini için yerel açıklamalar üretir. Makine öğrenmesi modellerinin iç mekanizmasını anlamaya yardımcı olur. Modelden bağımsızdır yani herhangi bir makine öğrenmesi modeline uygulanabilir. Veri örneklerini değiştirerek tahminlerin nasıl değiştirdiğini anlamaya çalışır. Seçilen gözlem etrafında, modelin tahminini açıklamak için öznitelikleri rastgele bir şekilde değiştirir. Değiştirilmiş özelliklerle birlikte, orijinal veri noktası model tarafından yeniden tahmin edilir.  Değiştirilmiş özniteliklerle yapılan tahminler arasındaki fark, özniteliklerin model tahminine olan katkısını ölçer. Bu, öznitelik ağırlıklarının hesaplanması için kullanılır.

\begin{itemize}
    \item \textbf{Local:} Yerel ağırlıklı doğrusal regresyon kullanır.
    \item \textbf{Interpretable:} Yerel açıklamalar sağlar.
    \item \textbf{Model Agnostic:} Modeli black box olarak ele alır. Modelden bağımsızdır.
\end{itemize}

\begin{lstlisting}[language=Python]
explainer = lime.lime_tabular.LimeTabularExplainer(
	X_train, 
	feature_names=feature_names, 
	class_names=iris.target_names, 
	discretize_continuous=True
)
exp = explainer.explain_instance(
	X_test[0], 
	model.predict_proba, 
	num_features=3, 
	top_labels=1
)
exp.show_in_notebook(show_table=True, show_all=False)
\end{lstlisting}

\subsection{SHAP}
SHAP (Shapley Additive Explanations) analizi, Shapley oyun teorisindeki mantığı kullanır. 1953 yılında Lloyd Shapley tarafından önerilmiştir. Shapley değerlerinin amacı, bir oyuncunun bir grup içindeki katkısını ölçmektir. Bir grup oyuncu, birlikte çalışarak belirli bir hedefe ulaşmaya çalışır ve her oyuncunun bu işbirliğine katkısı vardır. Shapley değerleri, bu katkıları ölçer ve adil bir şekilde paylaşımı sağlar. SHAP, her özniteliğin model tahminine ne kadar katkıda bulunduğunu ölçmek için Shapley Değerlerini kullanır. Shapley değerlerini hesaplamak için Kernel SHAP kullanılır. Her özniteliğin (özellik) değeri, diğer öznitelikler sabit tutularak rastgele permütasyonlarla değiştirilir.1. Her permütasyon sonrası modelin tahmini yeniden hesaplanır. Her öznitelik için yapılan değişikliklerin model tahminine olan etkisi ölçülür ve kaydedilir. Shapley değerleri, her öznitelik için yapılan değişikliklerin model tahminine olan etkilerinin ortalama alınmasıyla hesaplanır.

\begin{itemize}
    \item \textbf{Global Açıklanabilirlik:} Modelin tüm veri seti üzerinde tahminlerde bulunurken davranışını anlamak.
    \item \textbf{Yerel Açıklanabilirlik:} Modelin tek bir örnek üzerinde tahminde bulunurken davranışını anlamak.
\end{itemize}

\subsubsection{Grafikler}

\begin{itemize}
    \item \textbf{Bar Plot:} Değerler varsayılan olarak ortalama mutlak değer kullanılarak sıralanır ve y ekseninde azalan sırada gösterilir. Tüm veri seti ile veya tek bir örnek ile kullanılır.
    \item \textbf{Beeswarm Plot:} Y ekseni, genel önem sırasını azalan sırada gösterilir. Noktalar ise her bir örnekleme ait hesaplanan SHAP değerleridir. Tüm veri seti ile kullanılır.
    \item \textbf{Waterfall Plot:} X eksenindeki fonksiyon regresyon modelinin nihai çıktısıdır. Kırmızı barlar pozitifi etkiyi, mavi barlar negatif etkiyi gösterir. Y ekseninde değişken adları ve gönderilen örneğin değerleri yer almaktadır. Tek bir örnek ile kullanılır.
    \item \textbf{Dependence Plot:} Tek bir değişkenin model tarafından yapılan tahminler üzerindeki etkisini gösterir. X ekseni değişkenin değerini Y ekseni ise değişkenin değerinin Shapley değerini gösterir.
\end{itemize}

\begin{lstlisting}[language=Python]
explainer = shap.Explainer(model, X_train)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test, feature_names=feature_names)
\end{lstlisting}

\subsection{ELI5}
ELI5 (Explain like im 5 - 5 yaşındaymışım gibi açıkla), makine öğrenmesi modellerin açıklamaya yarıyan bir python kütüphanesidir. Modelden bağımsızdır. ELI5, her bir özelliğin bir tahmine ne kadar katkıda bulunduğunu gösteren özellik önem puanları sağlar.

\begin{lstlisting}[language=Python]
perm = PermutationImportance(model, random_state=42).fit(X_test, y_test)
eli5.show_weights(perm, feature_names=feature_names)
\end{lstlisting}

\subsection{Grad-CAM}
Grad-CAM (Gradient-weighted Class Activation Mapping), görüntü sınıflandırmada görüntünün hangi piksellerinin sınıflandırmada etkili olduğunu görselleştirmek için kullanılır. Modelin sınıflandırmada hangi kararları belirlediğini anlamak için gradyanların geriye yayılmasını (backpropagation) kullanır. Sınıf aktivasyon haritalama (CAM), modelin son katmanındaki aktivasyonları kullanarak modelin kararını etkileyen bölgelerin bir ısı haritası oluşturur. Grad-CAM, CAM'a ek olarak gradyan bilgilerini de kullanır.

\subsubsection{Çalışma Adımları}
\begin{enumerate}
    \item Çıktı olarak elde edilen özellik haritaları alınır.
    \item İlgilenilen sınıf için çıktıya göre gradyanlar hesaplanır.
    \item Özellik haritalarındaki her bir piksel için gradyanlarla ağırlıklı bir toplam hesaplanır. 
    \item Ağırlıklandırılmış toplamlar bir aktivasyon fonksiyonundan geçirilerek aktivasyon haritası oluşturulur. Bu harita, hangi bölgelerin modelin sınıflandırmasında etkili olduğunu gösterir.
    \item Aktivasyon haritası orijinal görüntü üzerine yerleştirilir.
\end{enumerate}

\newpage