\section{Byte Pair Encoding (BPE)}

Byte Pair Encoding, bir veri sıkıştırma ve alt kelime parçacıkları öğrenme tekniğidir. Aslen bir sıkıştırma algoritması olarak ortaya çıksa da dil modellerinde daha iyi kelime temsilleri oluşturmak amacıyla da yaygın olarak kullanılmaktadır. Özellikle dillerin morfolojik yapıları çok karmaşık olduğunda veya dil modellerinin öğrenmesi gereken büyük kelime dağarcığıyla başa çıkmak gerektiğinde BPE devreye girer. Bu yöntem sayesinde, dil modellleri çok nadir kelimelerle bile başa çıkabilir çünkü kelimeleri küçük alt birimlerine ayırır ve bu alt birimlerle daha iyi genelleme sağlar. 

\subsection{Çalışma Adımları}

BPE algoritması, en sık görülen bitişik sembol çiftlerini tekrar tekrar birleştirerek çalışır. İlk başta, her kelimeyi veya cümleyi karakterlerine ayırır. Ardından, belirli bir sayıda tekrarlanan sembol çiftlerini birleştirir ve bu işlem istenilen birleştirme sayısına ulaşana kadar devam eder. Bu süreç, kelimeleri daha küçük birimleri kullanarak temsil etmeyi sağlar.

\begin{enumerate}
    \item Metindeki her kelime veya sembol, bir liste olarak başlar.
    \item Semboller arasındaki en sık görülen çift bulunur.
    \item Bu sembol çifti tek bir sembol olarak birleştirilir.
    \item Bu adımlar belirli sayıda birleştirme gerçekleşene kadar tekrarlanır.
\end{enumerate}

\subsection{Örnek Kod}

\begin{lstlisting}[language=Python]
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("gpt2")

def tokenize(text):
    words = [word for word, _ in tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)]
    splits = [[char for char in word] for word in words]

    for pair, merge in merges.items():
        for split in splits:
            for i in range(len(split) - 1):
                if split[i:i+2] == list(pair):
                    split[i:i+2] = [merge]

    return sum(splits, [])

print(tokenize("This is not a token."))  
\end{lstlisting}

\newpage