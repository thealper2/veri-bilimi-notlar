\section{Extraction Attacks}

\subsection{Copycat CNN}

Copycat CNN, bir saldırganın olmadığı hedef bir modeli taklit etmek amacıyla, o modelin çıktıları üzerinden kendi benzer modelini eğitmesi anlamına gelir. Saldırgan, hedef modelin iç yapısını bilmeden sadece giriş-çıkış verilerini kullanarak benzer bir model üretmeye çalışır.

\subsubsection{Savunma Mekanizmaları}

\begin{itemize}
    \item \textbf{Differential Privacy}: Modelin çıktılarına rastgelelik ekleyerek, saldırganın tam bilgiye ulaşmasını zorlaştırmak.
    \item \textbf{Model Watermarking}: Modelin sahiplik bilgilerini dijital olarak içine gömmek, böylece taklit edildiği anlaşılabilir.
\end{itemize}

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.estimators.classification import TensorFlowV2Classifier
from art.attacks.extraction import CopycatCNN

copycat_classifier = TensorFlowV2Classifier(
    model=copycat_model, 
    loss_object=CategoricalCrossentropy(from_logits=True), 
    optimizer=Adam(learning_rate=0.01), nb_classes=10,
    input_shape=(28, 28), clip_values=(min_, max_)
)

copycat_attack = CopycatCNN(
    classifier=copycat_classifier, nb_epochs=10,
    nb_stolen=10000
)

copycat_attack.extract(x_test, y_test, thieved_classifier=target_classifier)
\end{lstlisting}

\newpage

\subsection{Functionally Equivalent Extraction}

Functionally Equivalent Extraction saldırıları, genellikle hedef modelin iç yapısını tamamen bilmese de aynı fonksiyonelliği yakalamayı amaçlar. Saldırgan, hedef modele birçok giriş verisi sağlar ve modelin verdiği çıktıları kullanarak bir model oluşturur. Bu saldırıda, hedef modelin sadece çıktıları gözlemlenir ve kendi modelini bu verilere dayalı olarak eğitir. Hedef model ile saldırganın ürettiği model aynı olmasa da fonksiyonel olarak birbirine çok yakın sonuçlar üretirler.

\begin{itemize}
    \item \textbf{Differential Privacy}: Modelin çıktılarına rastgelelik ekleyerek, saldırganın tam bilgiye ulaşmasını zorlaştırmak.
    \item \textbf{Model Watermarking}: Modelin sahiplik bilgilerini dijital olarak içine gömmek, böylece taklit edildiği anlaşılabilir.
\end{itemize}

\newpage

\subsection{Knockoff Nets}

Knockoff Nets saldırıları, ticari ve patentli makine öğrenimi modellerinin taklit edilmesine ve modellerin entelektüel mülkiyet haklarının ihlal edilmesine olanak sağlar. Bu saldırı, hedeflenen derin öğrenme modelinin iç yapısına erişim olmadan, sadece girdi ve çıktı verilerinden yola çıkarak hedef modelin bir kopyasını üretmeye dayanır. Saldırgan, hedef modelin sunduğu API veya servis üzerinden modelin davranışlarını inceleyerek kendi modelini oluşturur ve eğitir.

\begin{itemize}
    \item \textbf{Differential Privacy}: Modelin çıktılarına rastgelelik ekleyerek, saldırganın tam bilgiye ulaşmasını zorlaştırmak.
    \item \textbf{Model Watermarking}: Modelin sahiplik bilgilerini dijital olarak içine gömmek, böylece taklit edildiği anlaşılabilir.
    \item \textbf{Rate Limiting}: Sorgu sayısını sınırlayarak, saldırganın veri toplaması önlenebilir.
\end{itemize}

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.estimators.classification import TensorFlowV2Classifier
from art.attacks.extraction import KnockoffNets

knockoff_classifier = TensorFlowV2Classifier(
    model=knockoff_model, 
    loss_object=CategoricalCrossentropy(from_logits=True), 
    optimizer=Adam(learning_rate=0.01),
    nb_classes=10,
    input_shape=(28, 28),
    clip_values=(min_, max_)
)

knockoff_attack = KnockoffNets(
    classifier=knockoff_classifier, 
    nb_epochs=10,
    nb_stolen=10000
)

knockoff_attack.extract(x_test, y_test, thieved_classifier=target_classifier)
\end{lstlisting}

\newpage