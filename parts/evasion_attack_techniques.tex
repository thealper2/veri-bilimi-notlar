\section{Evasion Attacks}

\subsection{Adversarial Patch}

Adversarial Patch, saldırganın hedef modelin karar verme sürecini manipüle etmek amacıyla görüntülerin üzerine eklediği bir "yama" (patch) kullanılarak yapılır. Bu yama, görüntüye yerleştirildiğinde modelin yanlış sınıflandırmasına neden olur. Modelin içindeki sinir ağları, görsel özellikleri inceleyerek karar verir. Adversarial Patch, bu özellikleri yanıltarak, ağırlıkların yanlış hesaplanmasını sağlar ve modelin hatalı sınıflandırma yapmasına yol açar. Saldırgan, bir optimizasyon süreci kullanarak yamayı üretir. Yamayı oluşturan piksel değerleri, modelin doğru çalışmasını bozacak şekilde optimize edilir.

\[ \min_{\delta} \mathbb{E}_{x \sim \mathcal{D}} \left[ \mathcal{L}(f(x + \delta), y_{\text{target}}) \right] \]

Burada:

\begin{itemize}
    \item $x$: Giriş görüntüsü.
    \item $\delta$: Adversarial patch.
    \item $f(x + \delta)$: Patch eklenmiş görüntü ile modelin tahmini.
    \item $y_{target}$: Hedeflenen yanlış sınıf.
    \item $\mathcal{L}$: Kayıp fonksiyonu.
\end{itemize}

Diğer Evasion yöntemleri girdiye çeşitli bozulmalar ekleyerek modeli yanıltmayı amaçlar. Adversarial Patch ise görüntünün herhangi bir yerine yerleştirilebilir ve modelin diğer öğeleri görmezden gelmesin sağlayarak yanıltır. Diğer saldırılar dijital ortamda manipüle gerektirirken Adversarial Patch fiziksel dünyada da uygulanabilir. Sahneden bağımsızdır. Hedefli bir saldırıdır yani istenen bir çıktı vermeye zorlanabilir. Döndürme, ölçekleme gibi çeşitli savunma yöntemlerine karşı dirençlidir. Fakat saldırının etkili olabilmesi için oluşturulan yamanın belirli bir boyutta olması gerekir. Büyük bir yama, şüphe uyandırabilir ve saldırının başarısız olmasına neden olabilir. Bunu önlemek için "disguise" yani gizleme işlemi yapılmıştır. Disguise işlemi:

\begin{itemize}
    \item \textbf{L2 Mesafesi}: Oluşturulan yamanın bir "tie-dye" desenine olan L2 mesafesini en aza indirerek daha az dikkat çekici olması sağlanmıştır. Bu işlem, yamanın renklerini ve desenini değiştirerek onu "tie-dye" desenine benzemesini sağlar. Tie-dye, bir dizi boyama işlemi sonucu elde edilen görüntüye verilen isimdir.
    \item \textbf{Barış İşareti Maskesi (Peace Sign Mask)}: Yamaya barış işareti maskesi uygulanır. Bu maske, yamanın şeklini değiştirir.
\end{itemize}

\subsubsection{Çalışma Adımları}

\begin{enumerate}
    \item İlk olarak, hangi sınıflandırmanın yanlış yapılmasının istendiği belirlenir.
    \item Yamanın yerleştirileceği bölge ve büyüklük seçilir. Bu yamayı oluşturan piksel değerleri optimize edilecektir.
    \item Saldırgan, modelin tahmin hatası üzerinden yamayı optimize eder. Bunun için kullanılan optimizasyon algoritmaları yamayı, modele zarar verecek şekilde günceller.
    \item Optimizasyon tamamlandıktan sonra yamayı hedef görüntüye yerleştirir ve modelin saldırıya nasıl tepki verdiği gözlemlenir.
\end{enumerate}

\subsubsection{Savunma Yöntemleri}

\begin{itemize}
    \item \textbf{Aktivasyon Tespiti}: Görüntü üzerinde anormal derecede aktivasyona neden olan bölgeler tespit edilir.
    \item \textbf{Adversarial Training}: Eğitim verilerine adversarial örnekler eklenerek bu tür saldırılara direnç sağlanabilir.
    \item \textbf{Ensemble Models}: Birden fazla modelin kullanılarak sonuçlarının birleştirilmesiyle nihai tahmin yapılabilir.
    \item \textbf{İnsan Denetimi}: Adversarial Patch saldırısının başarısı uygulanan yamanın boyutu ile ilişkilidir. Dolayısıyla görüntü üzerindeki büyük bir yama insan gözüyle tespit edilebilir.
\end{itemize}

\newpage

\subsection{Adversarial Texture}

Adversarial Texture, bir nesnenin yüzeyine uygulanan ve o nesnenin model tarafından yanlış tanınmasına yol açan bir "doku" ile saldırı gerçekleştirir. Adversarial Texture, temelde optimizasyon süreçleri kullanarak nesnenin görünümünü manipüle eder. Modelin tahmin performansını bozacak şekilde tasarlanmış olan bu dokular, model tarafından işlenen özellikleri yanıltarak modelin yanlış tahminler yapmasına yol açar. Bu saldırı, görüntüdeki belirli bir dokuya odaklanarak yapıldığı için daha geniş ve nesnenin genel yapısını etkileyen bir saldırı olarak kabul edilir. Adversarial Texture, Expectation over Transformation (EoT) ile oluşturulur. 

\[ \min_{T} \mathbb{E}_{x \sim \mathcal{D}} \left[ \mathcal{L}(f(T(x)), y_{\text{target}}) \right] \]

Burada:

\begin{itemize}
    \item $x$: Giriş görüntüsü.
    \item $T(x)$: Manipüle edilmiş dokuya sahip görüntü.
    \item $f(T(x))$: Modelin dokulu görüntüyle verdiği tahmin.
    \item $y_{target}$: Hedeflenen yanlış sınıf.
    \item $\mathcal{L}$: Kayıp fonksiyonu.
\end{itemize}

\subsubsection{Çalışma Adımları}

\begin{enumerate}
    \item İlk olarak, hangi modelin ve hangi nesnenin hedefleneceği belirlenir.
    \item Saldırgan, modelin tahmin performansını bozacak şekilde dokuyu tasarlar.
    \item Doku, modelin sınıflandırma doğruluğunu bozacak şekilde optimize edilir. Bu süreçte kayıp fonksiyonu minimize edilir ve doku, hedeflenen yanlış sınıfı oluşturacak şekilde güncellenir.
    \item Optimizasyon sonucunda elde edilen doku, nesneye uygulanır ve modelin saldırıya nasıl tepki verdiği gözlemlenir.
\end{enumerate}

\newpage

\subsection{Auto Projected Gradient Descent (Auto-PGD)}

Auto-PGD, bir modelin saldırılara karşı dayanıklılığını (adversarial robustness) test etmek için kullanılan bir saldırı yöntemidir. Auto-PGD, PGD'nin iyileştirilmiş bir versiyonudur ve modelin güvenliğini daha etkin bir şekilde test etmek için otomatik olarak optimizasyon parametrelerini belirler. 

Auto-PGD'nin temel mantığı, modelin kayıp fonksiyonunun gradyanını hesaplayarak adversarial örnekler üretmektir. Bu süreç birkaç iterasyon boyunca devam eder ve üretilen adversarial örnekler, modelin kararlılığına zarar vererek modelin yanlış sınıflandırmalar yapmasını sağlar. Auto-PGD, belirlenen bir sınır içinde kalarak en etkin saldırıyı bulmaya çalışır.

\[ x^{t+1} = \text{Proj}_{\mathcal{B}(x, \epsilon)} \left( x^t + \alpha \cdot \frac{\nabla_x \mathcal{L}(f(x^t), y)}{\|\nabla_x \mathcal{L}(f(x^t), y)\|_p} \right) \]

Burada:

\begin{itemize}
    \item $x^t$: t-inci adımda üretilen adversarial örnek.
    \item $\alpha$: İterasyon başına yapılan adım büyüklüğü.
    \item $\mathcal{L}(f(x^t), y)\|$ Modelin kayıp fonksiyonunun giriş verisine göre gradyanı.
    \item $\text{Proj}_{\mathcal{B}(x, \epsilon)}$: Girişin $\epsilon$-norm sınırları içinde kalmasını sağlayan projeksiyon.
    \item $p$: Norm tipi (genellikle $L_\infty$ normu kullanılır.)
\end{itemize}

\subsubsection{Çalışma Adımları}

\begin{enumerate}
    \item İlk olarak saldırılacak model ve veri seti seçilir.
    \item Pertürbasyon için belirli bir sınır tanımlanır. Bu sınır, saldırının şiddetini belirler.
    \item Modelin kayıp fonksiyonunun giriş verisine göre gradyanı hesaplanır. Bu gradyan, hangi yönde saldırı yapılacağını gösterir.
    \item Hesaplanan gradyanlar kullanılarak giriş verisi üzerinde değişiklikler yapılır, ancak yapılan bu değişiklikler belirlenen $\epsilon$-norm sınırları içinde kalacak şekilde projeksiyon uygulanır.
    \item Bu süreç birkaç iterasyon boyunca tekrarlanır.
\end{enumerate}

\newpage

\subsection{Auto Conjugate Gradient (Auto-CG)}

Auto-CG, Conjugate Gradient (CG) optimizasyon yöntemine dayanır. Auto-CG, modelin kayıp fonksiyonunu minimize etmek için conjugate gradient yaklaşımını kullanır ve bu optimizasyon yöntemi ile adversarial örnekler üretir. Auto-CG, conjugate gradient algoritmasının otomatikleştirilmiş ve model güvenliği için optimize edilmiş bir versiyonudur. Conjugate Gradient (CG) yöntemi, gradyan tabanlı optimizasyon yöntemlerinin geliştirilmiş bir şeklidir. Bu yöntem, özellikle büyük boyutlu problem setlerinde daha hızlı ve daha etkin bir optimizasyon sağlar. CG algoritması, modelin kayıp fonksiyonunun gradyanını hesaplar ve conjugate (eşlenik) yönünde adım atarak hedef fonksiyonu minimize etmeye çalışır.

Auto-CG'nin temel formülü, conjugate gradient algoritmasındaki adım güncellemesini kullanır. İteratif olarak, her adımda giriş verisi $x$, conjugate direction (eşlenik yön) boyunca güncellenir.

\[ x^{t+1} = x^t + \alpha^t d^t \]

Burada:

\begin{itemize}
    \item $x^t$: t-inci iterasyondaki veri örneği.
    \item $\alpha^t$: İterasyon başına adım büyüklüğü.
    \item $d^t$: Conjugate (eşlenik) yön.
\end{itemize}

\subsubsection{Çalışma Adımları}

\begin{enumerate}
    \item Model ve veri seti seçilir.
    \item Saldırı için bir başlangıç örneği seçilir ve bu örnek üzerinde iterasyonlar yapılır.
    \item Modelin kayıp fonksiyonu, giriş veri üzerinde gradyanı hesaplayarak optimize edilir.
    \item Conjugate Gradient yöntemine uygun olarak eşlenik yön hesaplanır ve bu yön boyunca adım atılır.
    \item Her adımda daha etkili bir adversarial örnek üretilir.
    \item Yapılan değişikliklerin belirli bir $\epsilon$-norm içinde kalması için projeksiyon adımı uygulanır.
\end{enumerate}

\newpage

\subsection{Boundary Attack (Decision-Based Attack)}

Boundary Attack, karara dayalı bir saldırı yöntemidir. Bu saldırı, modelin iç yapısını veya gradyan bilgilerini kullanmadan, yalnızca modelin verdiği kararlar (etiketler) üzerinden çalışır. Boundary Attack, modelin yanlış sınıflandırmasını sağlamak için, verinin sınırına (decision boundary) en yakın olan yanlış sınıflandırılan örneği bulmaya çalışır. Bu yöntem, örneği kademeli olarak modelin karar sınırına iterek çalışır.

Boundary Attack, rastgele bir örnekten başlar ve bu örneğe küçük pertürbasyonlar ekleyerek, modelin karar sınırına en yakın yanlış sınıflandırılan örneği bulmaya çalışır. Bu yöntemle, modelin verdiği kararları gözlemleyerek (etiketlere dayanarak) örneği değiştirmeyi amaçlar.

\subsubsection{Çalışma Adımları}

\begin{enumerate}
    \item Saldırı, rastgele bir örnekten başlar. Bu, modelin karar sınırına uzak bir örnektir.
    \item Rastgele seçilen bu örneğin karar sınırına doğru ilerletilmesi gerekir. Bunun için, modelin sınıflandırma kararlarını kullanarak, veri noktasına küçük değişiklikler yapılır.
    \item Adversarial örnek, modelin yanlış sınıflandırılması gereken karar sınırına mümkün olduğunca yakın hale getirilmeye çalışılırken, aynı zamanda minimum düzeyde pertürbasyon eklenir.
    \item Her iterasyonda, pertürbasyon küçülür ve sınırdaki en küçük değişimlerle hedeflenen sınıflandırmaya ulaşılmaya çalışılır.
\end{enumerate}

\newpage

\subsection{Brendel and Bethge (B\&B) Attack}

B\&B Attack, karar tabanlı bir saldırı yöntemidir. Bu saldırı, modelin karar sınırına en yakın yanlış sınıflandırılan bir örnek bulmayı hedefler. Black-box bir saldırı türü olduğundan, saldırgan modelin gradyanlarına, iç yapısına veya ağırlıklarına erişime ihtiyaç duymaz, yalnızca modelin verdiği kararları (etiketleri) kullanır. Bu saldırı, özellikle modelin yanlış sınıflandırmasına yol açacak minimum pertürbasyonları üretmek amacıyla, modelin verdiği kararları optimize ederek çalışır. Boundary Attack ile benzerlik gösterse de, Brendel and Bethge Attack daha hassas ve ince ayarlı bir saldırı mekanizması kullanır.

Brendel and Bethge Attack, modelin kararlarına göre giriş verisine minimal değişiklikler yapar. Pertürbasyon $\delta$, giriş verisine $x$ şu şekilde uygulanır:

\[ x_{\text{adv}} = x + \delta \]

\subsubsection{Çalışma Adımları}

\begin{enumerate}
    \item İlk adımda, yanlış sınıflandırılan bir örnek seçilir.
    \item Modelin karar sınırına en yakın olan noktayı bulmak için küçük ve optimize edilmiş pertürbasyonlar eklenir. Bu adımda, modelin kararlarını gözlemleyerek minimum düzeyde değişiklikler yapılır.
    \item Pertürbasyonlar modelin verdiği kararlarla kıyaslanır ve her iterasyonda sınırdaki yanlış sınıflandırmayı bulmak hedeflenir.
    \item Her iterasyonda pertürbasyonlar optimize edilir, böylece yanlış sınıflandırılan en yakın sınırdaki örnek bulunmaya çalışılır.
\end{enumerate}

\newpage

\subsection{Carlini and Wagner L0 Attack}

C\&W L0 Attack, Carlini and Wagner tarafından geliştirilmiştir. Amacı, modellerin yanlış sınıflandırma yapmasına neden olmak için en az sayıda pikseli değiştirmektir. L0 normunu minimize eden bu saldırı, pertürbasyonların en az sayıda pikselde yapılmasını sağlar.

L0 normu, bir vektörde sıfır olmayan elemanların sayısını ifade eder. C\&W L0 saldırısı, görüntüdeki minimum sayıda pikseli değiştirerek modeli yanlış sınıflandırmaya zorlar. Bu, saldırının insan gözünde çok zor fark edilebilecek kadar küçük değişikliklerle yapılmasını sağlar.

\[ \min \|x' - x\|_0 \quad \text{subject to} \quad f(x') = t \]

Burada:

\begin{itemize}
    \item $x$: Giriş görüntüsü.
    \item $x'$: Adversarial örnek.
    \item $t$: Hedef sınıf.
    \item $\min \|x' - x\|_0$: L0 normu, değiştirilen piksel sayısı.
\end{itemize}

\subsubsection{Çalışma Adımları}

\begin{enumerate}
    \item İlk olarak, doğru sınıflandırılan bir görüntü alınır.
    \item Amaç ya bir hedef sınıfa yönelik saldırı gerçekleştirmek (targeted attack) ya da yanlış bir sınıflandırma yaratmaktır (non-targeted attack).
    \item Saldırı, giriş görüntüsüne eklenmesi gereken en küçük değişiklikliği bulmak için bir optimizasyon problemi çözer. Bu adımda, mümkün olan en az sayıda piksel değiştirilir ve bozulma sınırlanır.
    \item Yapılan değişiklikler, modelin yanlış sınıflandırmasını sağlayana kadar optimize edilir. Bu işlem sırasında saldırı, L0 normunu minimize eder.
\end{enumerate}

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.attacks.evasion import CarliniL0Method

cw_l0 = CarliniL0Method(
    classifier=classifier, 
    confidence=0.1, 
    batch_size=1, 
    learning_rate=0.01, 
    max_iter=10
)
adv_test = cw_l0.generate(x=x_test)
\end{lstlisting}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{images/cw_l0_results.png}
    \caption{}
\end{figure}

\newpage

\subsection{Carlini and Wagner L2 Attack}

C\&W L2 saldırısı, giriş görüntüsüne eklenen bozulmayı L2 normuna göre optimize ederek, modelin yanlış sınıflandırma yapmasına neden olur. L2 normu, giriş görüntüsüne en az bozulmayı ekleyerek modeli yanıltmayı amaçlar. Amaç, modelin sınıflandırmasını bozmak için görüntüye çok küçük ama stratejik değişiklikler yapmaktır. C\&W L2 saldırısı, bozulmaların toplam büyüklüğünü minimize eder ve bu sayede insan gözünün algılaması zor olan adversarial örnekler oluşturur.

\[ \min \|x' - x\|_2^2 + c \cdot f(x') \]

Burada:

\begin{itemize}
    \item $x$: Giriş görüntüsü.
    \item $x'$: Adversarial örnek.
    \item $\min \|x' - x\|_2$: L2 normu, yani giriş görüntüsü ile adversarial örnek arasındaki farkın büyüklüğü.
    \item $f(x')$: Modelin sınıflandırma kaybı.
    \item $c$: Kayıp fonksiyonunu dengeleyen bir sabit.
\end{itemize}

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.attacks.evasion import CarliniL2Method

cw_l2 = CarliniL2Method(
    classifier=classifier, 
    confidence=0.1, 
    batch_size=1, 
    learning_rate=0.01, 
    max_iter=10
)
adv_test = cw_l2.generate(x=x_test)
\end{lstlisting}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{images/cw_l2_results.png}
    \caption{}
\end{figure}

\newpage

\subsection{Carlini and Wagner L$\infty$ Attack}

C\&W L$\infty$, belirli bir giriş görüntüsüne maksimum bir bozulma ekleyerek modelin yanlış sınıflandırmasını sağlamayı amaçlar. L$\infty$ normu, bir görüntüdeki en büyük piksek değişikliğini sınırlayarak adversarial örnekler oluşturur. 

\[ \min \|x' - x\|_\infty + c \cdot f(x') \]

Burada:

\begin{itemize}
    \item $x$: Giriş görüntüsü.
    \item $x'$: Adversarial örnek.
    \item $\min \|x' - x\|_2$: L2 normu, yani giriş görüntüsü ile adversarial örnek arasındaki farkın büyüklüğü.
    \item $f(x')$: Modelin sınıflandırma kaybı.
    \item $c$: Kayıp fonksiyonunu dengeleyen bir sabit.
\end{itemize}

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.attacks.evasion import CarliniLInfMethod

cw_l_inf = CarliniLInfMethod(
    classifier=classifier, 
    confidence=0.1, 
    batch_size=1, 
    learning_rate=0.01, 
    max_iter=10
)
adv_test = cw_l_inf.generate(x=x_test)
\end{lstlisting}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{images/cw_l_inf_results.png}
    \caption{}
\end{figure}

\newpage

\subsection{Carlini and Wagner ASR Attack}

Otmatik konuşma sistemlerini hedef alır. Bu saldırı, konuşma tanıma sistemlerinin verdiği çıktıları manipüle etmek için adversarial ses sinyalleri üretir. Amacı, insanların algılayamayacağı seviyede ses bozulmaları oluşturarak bir ses kaydını, ASR sistemlerinde yanlış şekilde tanınan veya hedeflenen bir komut haline getirmektir.

\[ \min \|x' - x\|_2 + c \cdot f(x') \]

Burada:

\begin{itemize}
    \item $x$: Giriş sinyali.
    \item $x'$: Adversarial örnek.
    \item $\min \|x' - x\|_2$: L2 normu, yani giriş sinyali ile adversarial örnek arasındaki farkın büyüklüğü.
    \item $f(x')$: Modelin sınıflandırma kaybı.
    \item $c$: Kayıp fonksiyonunu dengeleyen bir sabit.
\end{itemize}

\subsubsection{Çalışma Adımları}

\begin{enumerate}
    \item Hedef model ve sinyal seçilir.
    \item Saldırı hedefinin yanlış tanımlanmış bir komut ya da belirli bir hedef komut olması beklenir.
    \item ASR sistemini yanlış sonuç vermeye yönlendirecek en küçük pertürbasyonlar hesaplanır.
    \item Bu bozulmalar, L2 normu veya başka bir noma göre optimize edilerek saldırı güçlendirilir.
\end{enumerate}

\newpage

\subsection{Composite Adversarial Attack}

Composite Adversarial Attack, farklı türdeki evasion saldırılarını bir araya getirerek çoklu saldırı stratejileridir. Birkaç farklı saldırı yöntemi ardışık olarak uygulanır ya da bir araya getirilir. Amacı, bir modelin sadece tek bir saldırıya karşı değil, birden fazla saldırıya karşı dayanıklılığını test etmektir. 

\[ x' = S_2(S_1(x)) \]

Burada:

\begin{itemize}
    \item $x$: Orijinal girdi.
    \item $S_1(x)$: İlk saldırı tarafından modifiye edilen girdidir.
    \item $S_2(x')$: İkinci saldırı tarafından modifiye edilen girdidir.
\end{itemize}

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.estimators.classification import TensorFlowV2Classifier
from art.attacks.evasion import FastGradientMethod, ProjectedGradientDescent

art_model = TensorFlowV2Classifier(
    model=model, 
    loss_object=SparseCategoricalCrossentropy(from_logits=True), 
    nb_classes=10, input_shape=(784,), clip_values=(0, 1))

pgd = ProjectedGradientDescent(estimator=art_model, targeted=False, 
                               eps=0.1, eps_step=0.01, max_iter=40)

x_test_adv_fgsm = fgsm.generate(x_test)
x_test_adv_composite = pgd.generate(x=x_test_adv_fgsm)
\end{lstlisting}

\newpage

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{images/composite_adversarial_attack_results.png}
    \caption{}
\end{figure}

\newpage

\subsection{Decision Tree Attack}

Decision Tree Attack, karar ağacının düğümlerini ve dallarını hedef alarak saldırı oluşturur. Bu saldırı türünde, giriş verileri yavaş yavaş değiştirerek, birden fazla düğümde farklı kararlar alınmasını sağlar. Böylelikle, modelin çıktısını yanlış bir yöne yönlendirmek için küçük ama etkili değişiklikler yapılır.

Bu saldırı komşu yaprak arama yöntemini kullanır. Komşu yaprak arama, karar ağacının karar alma sürecini manipüle ederek modelin yanlış sınıflandırmalar yapmasını sağlar. Komşu yaprak arama yöntemi, ilk olarak normal bir örneğin hangi yaprağa karşılık geldiğini belirler. Daha sonra bu yaprağın yakın komşularını arayarak farklı sınıf etiketlerine sahip yaprakları bulur. Başlangıç yaprağından hedef yaprağa doğru giden yol incelenerek hedef yolda belirtilen koşulları sağlayacak küçük değişiklikler yapar. Örnek, bu yoldaki koşullara göre değiştirilir ve modelin yanlış tahmin yapması sağlanır.

\subsubsection{Çalışma Adımları}

\begin{enumerate}
    \item İlk olarak, modelin karar ağacı yapısı analiz edilir ve modelin nasıl çalıştığı anlaşılır.
    \item Karar ağacının her bir dalı boyunca, giriş verilerinde küçük değişiklikler yaparak saldırgan örnekler oluşturulur.
    \item Giriş verileri optimize edilerek modelin verdiği kararı yanlış yöne kaydıracak en iyi adversarial örnek bulunur.
    \item Karar ağacındaki zayıf noktalar tespit edildikten sonra, model bu zayıflıkları kullanarak saldırıya uğratılır.
\end{enumerate}

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.estimators.classification import SklearnClassifier
from art.attacks.evasion import DecisionTreeAttack
art_dt = SklearnClassifier(model=dt)
attack = DecisionTreeAttack(art_dt)
adv_test = attack.generate(x_test)
\end{lstlisting}

\newpage

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{images/dt_attack_results.png}
    \caption{}
\end{figure}

\newpage

\subsection{Deep Fool}

DeepFool, giriş verilerini sınırlı miktarda değiştirerek (pertürbe ederek), bu verilerin sınıflandırılmasını değiştirmeye çalışır. Amaç, orijinal sınıf ile yanlış sınıf arasında minimum değişikliklerle adversarial bir örnek oluşturmaktır. DeepFool, temelde karar sınırlarına en yakın olan minimum pertürbasyonu bulur ve bu pertürbasyonla sınıflandırmayı değiştirir.

\[ r = \frac{| f(x) |}{\|\nabla f(x)\|} \]

Burada:

\begin{itemize}
    \item $x$: Giriş verisi.
    \item $f(x)$: Modelin karar fonksiyonu.
    \item $\nabla f(x)$: Karar fonksiyonunun gradyanı.
    \item $r$: Giriş verisine eklenen pertürbasyon.
\end{itemize}

\subsubsection{Çalışma Adımları}

\begin{enumerate}
    \item İlk olarak modelin doğru sınıflandırdığı bir giriş verisi alınır.
    \item Modelin karar sınırları tespit edilir ve sınıf değiştirmek için gerekli minimum pertürbasyon hesaplanır.
    \item Hesaplanan küçük bir pertürbasyon giriş verisine eklenir.
    \item Bu işlem tekrarlanarak modelin sınıflandırmasını değiştirecek minimum pertürbasyon elde edilir.
\end{enumerate}

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.estimators.classification import TensorFlowV2Classifier
from art.attacks.evasion import DeepFool

art_model = TensorFlowV2Classifier(
    model=model, 
    loss_object=SparseCategoricalCrossentropy(from_logits=True), 
    nb_classes=10, input_shape=(784,), clip_values=(0, 1))

deep_fool = DeepFool(classifier=art_model)
adv_test = deep_fool.generate(x=x_test)
\end{lstlisting}

\newpage

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{images/deep_fool_attack_results.png}
    \caption{}
\end{figure}

\newpage

\subsection{DPatch}

DPatch Attack, sahnede belirli bir konuma küçük bir pertürbasyon yerleştirir ve bu sayede görüntüdeki diğer nesneler veya özellikler ne kadar net olursa olsun modelin çıktısını manipüle edebilir. Bu patch, modelin karar verme sürecini etkiler ve doğru sınıflandırmayı engeller. Bu saldırı, modelin girişindeki patch'e odaklanmasını sağlar ve bu sayede diğer özellikler göz ardı edilir.

Geleneksel Adversarial Patch, YOLO gibi Object Detection modellerinde düzgün çalışmaz. Bunun sebebi, object detection modellerinin önce nesneleri bulup sonra sınıflandırmasıdır. Dolayısıyla üretilen yamayı bir nesne olarak bulamadığı için sınıflandıramaz, bu da saldırının başarısız olmasına neden olur. DPatch, hem non-max suppression algoritmasını hem de sınıflandırıcıyı hedef alır. Oluşturulacak yama, hedeflenen nesnenin boyutuna göre olmalıdır. Yani ufak bir nesne hedef alınıyorsa o ölçekte bir yama oluşturulmalıdır. 

DPatch, aktarılabilir bir saldırıdır. Örneğin YOLO ile eğitilen bir model Faster R-CNN üzerinde de başarılı bir şekilde çalışabilir. Bu da saldrınının "black box" olması anlamına gelir. 

\subsubsection{Çalışma Adımları}

\begin{enumerate}
    \item Rastgele piksel değerlerine sahip bir yama oluşturulur.
    \item Oluşturulan yama veri setindeki görüntülere rastgele konumlarda eklenir.
    \item Model, yama eklenmiş görüntülerdeki nesneleri sınıflandırmaya çalışır.
    \item Modelin çıktısı gerçek etiketlerle karşılaştırılır ve bir kayıp değeri hesaplanır. Bu kayıp değeri kullanılarak yamanın piksel değeri güncellenir.
    \item Bu işlemler belirli bir iterasyon boyunca tekrarlanarak nihai yama oluşturulur.
\end{enumerate}

\newpage

\subsection{Robust DPatch}

Robust DPatch Attack, DPatch'in daha gelişmiş ve dayanıklı bir versiyonudur. Robust DPatch Attack, görüntü üzerinde belirli bir bölgeye (patch) küçük ama etkili bir pertürbasyon ekler ve bu patch, modelin tahminlerini saptırmak için optimize edilir. Bu patch, modelin dikkati başka yerlerde olsa dahi sınıflandırma kararını etkiler. Bu saldırı, modelin kararlarını manipüle etmek için optimizasyon sürecini kullanır ve "patch" boyutunu küçük tutarak saldırının fark edilmemesini sağlar.

\[ \min_\delta L(f(x + \delta), y_{\text{target}}) \]

Burada:

\begin{itemize}
    \item $L$: Kayıp fonksiyonu.
    \item $f$: Modelin tahmin fonksiyonu.
    \item $x$: Orijinal görüntü.
    \item $\delta$: Patch tarafından eklenen pertürbasyon.
    \item $y_{target}$: Hedeflenen yanlış sınıf.
\end{itemize}

\newpage

\subsection{ElasticNet Attack}

ElasticNet Attack, hem $L_1$ hem de $L_2$ normlarını kullanarak saldırı yapılacak pertürbasyonun seyrekliğini ve büyüklüğünü kontrol eder. Carlini and Wagner'in bir uzantısı olarak görülen bu saldırı, çok sayıda sıfır değerli (sparse) bileşen içeren pertürbasyonlar oluşturmayı amaçlar. $L_1$-norm seyrekliği, $L_2$-norm ise pertürbasyonun büyüklüğünü kontrol eder. Bu saldırı ISTA (Iterative Shrinkage-Thresholding Algorithm) yöntemini kullanır. Shrinkage işlemi, adversarial örneğin piksel değerini orijinal görüntüye yaklaştırır. Eşikleme işlemi, belirli bir eşik değerinin altındaki pertürbasyonları sıfırlar.

\[ \min_\delta \lambda \|\delta\|_1 + (1 - \lambda) \|\delta\|_2^2 + c \cdot f(x + \delta) \]

Burada:

\begin{itemize}
    \item $\delta$: Pertürbasyon.
    \item $\|\delta\|_1$: Pertürbasyonun $L_1$-normu (seyreklik ölçütü).
    \item $\|\delta\|_2^2$: Pertürbasyonun $L_2$-normu (pertürbasyonun büyüklük ölçütü).
    \item $f(x + \delta)$: Modelin tahmin fonksiyonu.
    \item $\lambda$: $L_1$ ve $L_2$ normları arasında dengeyi sağlayan katsayı.
    \item $c$: Sınıflandırma hatasını cezalandıran sabit.
\end{itemize}

\newpage

\subsection{Fast Gradient Method (FGM)}

FGM, modelin gradyanlarını kullanarak giriş verisine ufak bir pertürbasyon ekler. Eklenen bu pertürbasyon, modein tahminini yanlış sınıfa kaydırmaya çalışır. Fast Gradient Sign Method (FGSM) ise bu saldırının bir varyasyonudur ve pertürbasyonu sadece girdinin işareti yönünde ekleyerek gerçekleştirilir. Saldırının büyüklüğünü kontrol etmek için $\epsilon$ parametresi kullanılır. $\epsilon$, pertürbasyonun ne kadar büyük olacağını belirler.

\[ x_{adv} = x + \epsilon \cdot \text{sign}(\nabla_x J(\theta, x, y)) \]

Burada:

\begin{itemize}
    \item $x_{adv}$: Adversarial girdi.
    \item $x$: Orijinal girdi.
    \item $\epsilon$: Pertürbasyon büyüklüğünü belirleyen bir parametre.
    \item $\nabla_x J(\theta, x, y)$: Modelin kayıp fonksiyonunun girdi üzerindeki gradyanı.
    \item $\text{sign}$: Gradyanın işaretini belirten fonksiyon.
\end{itemize}

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.estimators.classification import TensorFlowV2Classifier
from art.attacks.evasion import FastGradientMethod

art_model = TensorFlowV2Classifier(
    model=model, 
    loss_object=SparseCategoricalCrossentropy(from_logits=True), 
    nb_classes=10, input_shape=(784,), clip_values=(0, 1))

fgsm = FastGradientMethod(estimator=art_model, eps=0.1)
adv_test = fgsm.generate(x=x_test)
\end{lstlisting}

\newpage

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{images/fgm_attack_results.png}
    \caption{}
\end{figure}

\newpage

\subsection{Feature Adversaries}

Feature Adversaries, modelin iç özellik uzayında yanıltıcı değişiklikler yapmayı amaçlayan bir saldırı türüdür. Diğer evasion saldırıları, modelin giriş uzayında küçük pertürbasyonlar ekleyerek modelin sınıflandırma doğruluğunu bozmayı hedeflerken, Feature Adversaries, modelin ara katmanlarındaki gradyan bilgilerini kullanarak giriş verisine küçük pertürbasyonlar eklemektedir. Bu pertürbasyonlar, doğrudan ara katmandaki özellik vektörünü hedef alır ve modelin çıktısında değişiklik yapar. Bu saldırı, özellik (feature) çıkarımına dayalı derin öğrenme modellerini hedef alır ve modele olan saldırıları daha hassas hale getirmek için ara katmanları kullanarak saldırganın etkisini artırır.

\[ x_{adv} = x + \epsilon \cdot \nabla_x J(f_l(x), y) \]

Burada:

\begin{itemize}
    \item $x_{adv}$: Adversarial örnek.
    \item $x$: Orijinal giriş.
    \item $\epsilon$: Pertürbasyonun büyüklüğünü kontrol eden bir parametre.
    \item $\nabla_x J(f_l(x), y)$: Modelin $l$. katmanındaki özellik vektörüne göre girdi üzerindeki gradyanı.
    \item $f_l(x)$: Modelin $l$. ara katmanındaki özellik çıkarımı.
\end{itemize}

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.estimators.classification import TensorFlowV2Classifier
from art.attacks.evasion import FeatureAdversariesTensorFlowV2

art_model = TensorFlowV2Classifier(
    model=model, 
    loss_object=CategoricalCrossentropy(from_logits=True), 
    nb_classes=10, input_shape=(28, 28, 1), clip_values=(0, 1))

valid_indices = np.where(
    np.argmax(y_test, axis=1)[:100] != np.argmax(y_test, axis=1)[100:200]
)[0]
source = x_test[:100][valid_indices][:32]
guide = x_test[100:200][valid_indices][:32]

feat_adv = FeatureAdversariesTensorFlowV2(
    art_model, layer=-2, delta=25/255,
    optimizer=None, step_size=1/255, max_iter=100,
)

adv_test = feat_adv.generate(source, guide)
\end{lstlisting}

\newpage

\subsection{Frame Saliency Attack}

Frame Saliency Attack, video çerçeveleri üzerinde küçük değişiklikler yaparak modelin yanlış sınıflandırma yapmasını sağlamayı amaçlar. Bu saldırı, çerçeve bazı hassasiyet haritalarını kullanarak hangi çerçevelerin modele daha fazla bilgi sağladığını tespit eder. Ardından, bu hassas çerçevelerdfe manipülasyonlar yaparak modelin davranışını bozmayı hedefler. Video içindeki her çerçeve için modelin dikkate aldığı önemli bölgeler (saliency map) hesaplanır. Bu, modelin hangi çerçeveleri veya bölgeleri daha çok dikkate aldığını gösterir. Hassas çerçeveler, modelin tahminini önemli ölçüde etkileyen çerçevelerdir. Bu çerçeveler üzerinde saldırı yapılması planlanır.

Frame Saliency Attack, bir video $V$ üzerindeki her çerçeve $F_t$ için bir saliency score $S(F_t)$ hesaplar. Modelin çıktısı $\hat{y}$:

\[ \hat{y} = f(V) \]

Burada $f$ video sınıflandırma modelidir ve $V = [F_1, F_2, ..., F_T]$ videodaki $T$ çerçeveyi temsil eder. Saliency haritası $M_t$, modein tahminine en çok katkıda bulunan çerçeveyi bulmak için kullanılır.

\[ S(F_t) = \sum_{i,j} \| \frac{\partial \hat{y}}{\partial F_t(i, j)} \| \]

Bu formülde $\frac{\partial \hat{y}}{\partial F_t(i, j)}$, çerçevenin her bir pikselinin model tahminine etkisini ifade eder. Ardından en yüksek saliency değerine sahip çerçeveler manipüle edilir.

\subsubsection{Çalışma Adımları}

\begin{enumerate}
    \item Her çerçeve için saliency map (hassasiyet haritası) hesaplanır. 
    \item Çerçevelere saliency puanı atanır ve en yüksek puanlı çerçeveler seçilir.
    \item Seçilen çerçeveler üzerinde küçük, insan tarafından fark edilemeyen değişiklikler yapılır.
    \item Manipüle edilen video modele verilir ve çıktının ne kadar değiştiği analiz edilir.
\end{enumerate}

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.attacks.evasion import FrameSaliencyAttack, FastGradientMethod
from art.estimators.classification import TensorFlowV2Classifier

classifier = TensorFlowV2Classifier(
    model=model,
    loss_object=CategoricalCrossentropy(from_logits=True),
    nb_classes=10,
    input_shape=(32, 32, 3),
)

fgsm = FastGradientMethod(estimator=classifier, eps=0.4)
attack = FrameSaliencyAttack(classifier=classifier, attacker=fgsm)
adv_test = attack.generate(x_test[:100])
\end{lstlisting}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{images/frame_saliency_attack_results.png}
    \caption{}
\end{figure}

\newpage

\subsection{Geometric Decision Based Attack (GeoDA)}

Geometric Decision-Based Attack, bir modelin karar sınırlarını öğrenip bu sınırlara yakın bir noktada gürültü veya küçük değişiklikler ekleyerek modelin yanlış bir sınıflandırma yapmasını sağlar. Saldırgan, modelin nasıl çalıştığını bilmeden yalnızca verdiği tahminler üzerinden saldırıyı gerçekleştirir.

Derin sinir ağlarının karar sınırları, verilerin yakınında düşük ortalama eğriliğe sahiptir. Yani, veri noktalarının yakınında karar sınırı neredeyse düz bir hiperdüzleme benzer. GeoDA, bu özelliği kullanarak, sınıflandırıcaya sınırlı sayıda sorgu yaparak hiperdüzleme dik olan bir vektör tahmin eder. Bu vektör, veri noktasını yanlış bir sınıfta tahmin etmek için gereken en az pertürbasyonu belirlemek için kullanılır.

Saldırı, başlangıçta doğru sınıflandırılan $x_0$ ve yanlış sınıflandırılan $x_1$ verileriyle başlar. Amaç, $x_0$ ile $x_1$ arasında bir doğru üzerinde yer alan karar sınırına yakın $\tilde{x}$ noktasını bulmaktır. Bir saldırı noktasi $x_i$ şu şekilde tanımlanır:

\[ x_i  = (1 - \lambda) x_0 + \lambda x_1 \]

Burada $\lambda$, karar sınırına olan yakınlığı belirler. Amaç, $\lambda$'yı optimize ederek en yakın karar sınırını bulmaktır.

\[ \text{min} || f(x_i) - f(x_0) || \]

Burada $f(x)$ modelin tahmin fonksiyonudur ve saldırının amacı, $f(x_i)$'nin $f(x_0)$'dan yeterince farklı hale getirilmesidir.

\subsubsection{Çalışma Adımları}

\begin{enumerate}
    \item Hedef modelin doğru sınıflandırdığı bir giriş noktası bulunur. Ardından bu noktanın etrafında, karar sınırına en yakın yanlış sınıflandırma yapan bir giriş noktası bulunur.
    \item Modelin karar sınırına en yakın olan bu yanlış sınıflandırılmış giriş noktası kullanılarak, bir geometrik yol izlenir. Amaç, saldırının etkili olabilmesi için karar sınırının en yakınında minimum değişiklikle saldırı yapmaktır.
    \item Her iterasyonda saldırı noktası güncellenir.
\end{enumerate}

\newpage

\subsection{GRAPHITE}

GRAPHITE, graph-based techniques (graf tabanlı teknikler) kullanılarak modelin karar sınırlarını etkili bir şekilde keşfetmeyi ve bu sınırları istismar etmeyi hedefler. Saldırganlar, grafik tabanlı veri yapılarında düğümler veya kenarlar üzerinde küçük değişiklikler yaparak, modelin doğru tahmin yapmasını engelleyebilir. GRAPHITE saldırıları hem black-box (modelin iç yapısı bilinmez, sadece çıktılar üzerinden saldırı yapılır) hem de white-box (modelin iç yapısı bilinir ve model parametreleri üzerinden saldırı yapılır) senaryolarında uygulanabilir.

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.attacks.evasion import GRAPHITEBlackbox
from art.estimators.classification import TensorFlowV2Classifier

classifier = TensorFlowV2Classifier(
    model=model,
    loss_object=CategoricalCrossentropy(from_logits=True),
    nb_classes=10, input_shape=(32, 32, 3),
    preprocessing=(0.5, 1), clip_values=(min_, max_)
)
attack = GRAPHITEBlackbox(
    classifier=classifier,  noise_size=(32, 32), 
    net_size=(32, 32),  heat_patch_size=(4, 4),
    heatmap_mode = 'Target',
)
x_adv = attack.generate(
    x=x_test[1, :, :, :][np.newaxis, :, :, :], 
    y=y_test[8][np.newaxis, :], 
    mask=None, x_tar=x_test[8,:,:,:][np.newaxis,:,:,:]
)
\end{lstlisting}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{images/graphite_results.png}
    \caption{}
\end{figure}

\newpage

\subsection{High Confidence Low Uncertainly (HCLU) Attack}

HCLU, bir modelin yüksek güven ile yanlış bir sınıflandırma yapmasını sağlarken, aynı zamanda modelin bu yanlış sınıflandırmada düşük belirsizlik taşımasını hedefler. HCLU saldırısı, özellikle belirsizlik ölçümlerini veya güven ölçümlerini kullanan makine öğrenimi modellerini hedef alır. Bu tür modeller, yalnızca tahminler yapmakla kalmaz, aynı zamanda tahminlerinin doğruluğuna ne kadar güvendiklerini de belirtirler. HCLU saldırısında, bir saldırgan, modelin güvenini istismar ederek, modele yanlış bir tahmin yaptırır ve model bu tahminin doğruluğuna "emin" görünür.

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.attacks.evasion import HighConfidenceLowUncertainty

attack1 = HighConfidenceLowUncertainty(classifier, conf=0.8, min_val=-1.0, max_val=2.0)
adv_confidence = attack1.generate(X_test)

attack2 = HighConfidenceLowUncertainty(classifier, unc_increase=0.9, min_val=0.0, max_val=2.0)
adv_uncertainty = attack2.generate(X_test)
\end{lstlisting}

\newpage

\subsection{Hop Skip Jump Attack (HSJA)}

HSJA, sınırlı erişim (black-box) altında çalışan bir saldırıdır ve decision boundary keşfi ile çalışır. HSJA, modelin karar sınırlarını (decision boundary) keşfetmek için sorgulamalarda bulunarak, adversarial (yanıltıcı) örnekler üretir. Modelin sınıflandırma sonuçlarına dayanarak, girdiyi modelin karar sınırına doğru iter ve ardından bu sınırı geçip yanlış sınıflandırma yapan bir adversarial örnek üretir.

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.attacks.evasion import HopSkipJump
from art.estimators.classification import TensorFlowV2Classifier

classifier = TensorFlowV2Classifier(
    model=model,
    loss_object=CategoricalCrossentropy(from_logits=True),
    nb_classes=10,
    input_shape=(32, 32, 3),
    preprocessing=(0.5, 1),
    clip_values=(min_, max_)
)

attack = HopSkipJump(classifier=classifier, targeted=False, max_iter=0, max_eval=1000, init_eval=10)
iter_step = 10
init_image = x_test[1]
target_image = x_test[6]
x_adv = np.array([init_image])
mask = np.random.binomial(n=1, p=0.9, size=np.prod(target_image.shape))
mask = mask.reshape(target_image.shape)

for i in range(20):
    x_adv = attack.generate(x=np.array([target_image]),
                            y=y_test[1],
                            x_adv_init=x_adv, 
                            resume=True, 
                            mask=mask)

    print(f"[Step {(i+1) * iter_step}/200] [L2 Error: {np.linalg.norm(np.reshape(x_adv[0] - target_image, [-1])):.2f}]")
    attack.max_iter = iter_step

    plt.figure()
    plt.imshow((x_adv[0] * 255).astype(np.uint))
    plt.show()
\end{lstlisting}

\newpage

\subsection{Imperceptible ASR Attack}

Imcerceptible ASR, bir ses kaydına (audio input) insan kulağının algılayamayacağı şekilde küçük ve fark edilmesi zor değişiklikler ekleyerek, ses tanıma sistemini yanıltmaktır. Imperceptible (algılanamaz) denmesinin sebebi, yapılan değişikliklerin, insanın duyusal sınırlarının ötesinde olmasıdır. Yani, orijinal ses dosyasındaki değişiklikler insanlar tarafından fark edilemez, ancak ASR sistemleri bu küçük değişiklikler yüzünden yanlış metin çıktısı üretir. Gradient tabanlı bir optimizasyon yöntemi kullanılarak, ses sinyaline küçük değişiklikler eklenir. Bu değişiklikler, insan kulağıyla fark edilmeyecek kadar küçüktür, ancak ASR sisteminin metin çıktısını değiştirebilir.

\newpage

\subsection{Basic Iterative Method (BIM)}

BIM, aynı zamanda Iterative Fast Gradient Sign Method (IFGSM) olarak da bilinir. Tek adımlı FGSM saldırısının tekrarlanmış bir versiyonudur. BIM, orijinal giriş verisine (örneğin, bir görüntüye) küçük ve tekrarlı perturbasyonlar ekleyerek modelin yanlış sınıflandırma yapmasını sağlar. Her iterasyonda, giriş verisine FGSM ile hesaplanan bir gürültü eklenir ve bu işlem, belirli bir maksimum perturbasyon seviyesine ulaşılana kadar tekrarlanır.

\subsubsection{Çalışma Adımları}

\begin{enumerate}
    \item İlk olarak, doğru şekilde sınıflandırılan bir giriş verisi seçilir.
    \item $\epsilon$, toplam pertürbasyonun maksimum büyüklüğünü belirler. $\alpha$, her iterasyonda eklenen pertürbasyonun adım büyüklüğünü belirler.
    \item Her iterasyonda, giriş verisine küçük bir pertürbasyon eklenir. Pertürbasyon, modelin kaybını maksimize edecek şekilde hesaplanır.
    \item Yeni giriş verisi $\epsilon$ sınırları içinde kliplenir, böylece pertürbasyonun toplam büyüklüğü kontrol altında tutulur.
    \item Tüm iterasyonlar tamamlandığında, son haliyle adversarial örnek elde edilir.
\end{enumerate}

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.attacks.evasion import BasicIterativeMethod
from art.estimators.classification import TensorFlowV2Classifier

classifier = TensorFlowV2Classifier(
    model=model,
    loss_object=CategoricalCrossentropy(from_logits=True),
    nb_classes=10,
    input_shape=(32, 32, 3),
)

attack = BasicIterativeMethod(estimator=classifier, eps=0.3, eps_step=0.01, max_iter=10)
adv_test = attack.generate(x=x_test[:100])
\end{lstlisting}

\newpage

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{images/basic_iterative_method_results.png}
    \caption{}
\end{figure}

\newpage

\subsection{Projected Gradient Descent (PGD)}

PGD, bir modeli hedef alan düşman örnekleri üretmek için yönlendirilmiş gradyan inişi (gradient descent) tekniğini kullanır. Amaç, modelin yanlış tahmin yapmasını sağlamak için giriş verisinin üzerine küçük ama stratejik bozulmalar eklemektir. PGD, bu bozulmaları optimize etmek için birkaç adımda küçük güncellemeler yaparak, saldırıyı daha güçlü hale getirir. PGD, aslında Basic Iterative Method (BIM) saldırısının genel bir formu olup, giriş değerlerini belirli bir norm içinde tutmak için projeksiyon kullanır.

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.estimators.classification import SklearnClassifier
from art.attacks.evasion import ProjectedGradientDescent

art_model = SklearnClassifier(model=log_reg)
pgd = ProjectedGradientDescent(estimator=art_model, eps=0.3, 
                               eps_step=0.1, max_iter=40, targeted=False)
adv_test = pgd.generate(x_test)
\end{lstlisting}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{images/pgd_attack_results.png}
    \caption{}
\end{figure}

\newpage

\subsection{Laser Attack}

Laser Attack, sensörlerin çalışma prensibini manipüle ederek işlev görür. Lazer ışınları, bir sensöre veya kameraya yönlendirilir ve bu sensörlerin topladığı verilerde gürültü oluşturur. Sensörün algıladığı lazer ışınları, gerçek dünyadaki nesnelerin algılanmasını engelleyebilir veya yanlış algılamalar yaratabilir.

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.attacks.evasion.laser_attack.laser_attack import LaserBeamAttack

lb_attack = LaserBeamAttack(
    estimator = model,
    iterations = 50,
    max_laser_beam=(780, 3.14, 32, 32)
)

adv_images = []

for i in range(5):
    adv_image = lb_attack.generate(
        x=np.expand_dims(x_test[i], axis=0)
    )
    adv_images.append(adv_image)

adv_pred = []

for i in range(5):
    pred = model.predict(np.expand_dims(adv_images[i][0], 0))
    adv_pred.append(pred)
\end{lstlisting}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{images/evasion_laser_beam.png}
    \caption{}
\end{figure}

\newpage

\subsection{LowProFool Attack}

LowProFool saldırısı, girdiye uygulanan perturbasyonların düşük profilli olmasını sağlar, yani insan gözünün algılayamayacağı düzeyde minimal değişiklikler yapılır. Bu saldırının başarısı, modelin bu küçük değişikliklerle yanıltılmasında yatar.

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.attacks.evasion import LowProFool

attack = LowProFool(
    classifier=classifier,
    n_steps=10,
    eta=10,
    lambd=1.65,
    eta_decay=0.95
)
attack.fit_importances(x_train, y_train)

adv_test = attack.generate(x_test, y=np.eye(n_classes)[y_test])
\end{lstlisting}

\newpage

\subsection{NewtonFool}

Newton Fool saldırısının temel prensibi, bir girdi örneğini modelin karar sınırına iterken, Newton yönteminden yararlanarak en kısa yolu bulmaktır. Yani, modelin karar sınırına en yakın noktayı bulur ve bu noktaya ulaşacak kadar küçük perturbasyonlar ekleyerek giriş verisini adversarial bir örneğe dönüştürür.

\[ x_{k+1} = x_k - \frac{f(x_k)}{\nabla f(x_k)} \]

Burada:

\begin{itemize}
    \item $x_{k+1}$: Bir sonraki iterasyondaki girdi.
    \item $x_k$: Mevcut iterasyondaki girdi.
    \item $f(x_k)$: Sınıflandırıcı modelin çıktısı.
    \item $\nabla f(x_k)$: Sınıflandırıcı modelin çıktısına göre giriş üzerindeki gradyanı.
\end{itemize}

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.estimators.classification import TensorFlowV2Classifier
from art.attacks.evasion import NewtonFool

art_model = TensorFlowV2Classifier(
    model=model, 
    loss_object=SparseCategoricalCrossentropy(from_logits=True), 
    nb_classes=10, input_shape=(784,), clip_values=(0, 1))
deep_fool = NewtonFool(classifier=art_model)
adv_test = deep_fool.generate(x=x_test)
\end{lstlisting}

\newpage

\subsection{Malware Gradient Descent (MGD)}

MGD, gradyan inişi algoritmasını kullanarak, bir zararlı yazılımın özelliklerini minimum değişiklikle moedlin yanlış sınıflandırmasını sağlayacak şekilde değiştirir. Amacı, girdi verisi olan zararlı yazılımı modelin karar sınırına en yakın yanlış sınıflandırma yapılacak noktaya çekmektir.

\[ x_{t+1} = x_t - \alpha \cdot \nabla J(x_t, y) \]

Burada:

\begin{itemize}
    \item $x_t$: Mevcut zararlı yazılım örneği.
    \item $\alpha$: Öğrenme oranı.
    \item $\nabla J(x_t, y)$: Zararlı yazılım örneğinin kayıp fonksiyonu J'nin gradyanı.
    \item $y$: Hedeflenen sınıf.
\end{itemize}

\newpage

\subsection{Over The Air Flickering Attack (OTA Flickering Attack)}

OTA Flickering Attack, kamera veya sensör tabanlı algılama sistemlerine yönelik görsel saldırılar yapar. Bir sahnenin veya bir nesnenin üzerine hızlı şekilde yanıp sönen (flickering) sinyaller yerleştirerek, algılama modellerini yanıltmayı hedefler. Bu sinyaller, insan gözüyle fark edilemeyen küçük değişikliklerdir, ancak kameralar tarafından algılanır ve bu sayede modelin kararlarını değiştirmek için kullanılır.

\newpage

\subsection{Pixel Attack}

Pixel Attack, hedef görüntüde sınırlı sayıda pikseli değiştirerek modelin sınıflandırma kararını değiştirmeyi amaçlar. Bu saldırı, görüntünün genel yapısını bozmadan küçük bir değişiklik yaparak saldrının fark edilmemesini sağlar.

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.estimators.classification import TensorFlowV2Classifier
from art.attacks.evasion import PixelAttack

art_model = TensorFlowV2Classifier(model=model, nb_classes=10, 
                                   input_shape=(32, 32, 3), clip_values=(0, 1))
pixel_attack = PixelAttack(classifier=art_model, th=None, es=1)
adv_test = pixel_attack.generate(x=x_test[:16])
\end{lstlisting}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{images/pixel_attack_results.png}
    \caption{}
\end{figure}

\newpage

\subsection{Threshold Attack}

Threshold Attack, modelin çıkışının (olasılık veya karar fonksiyonu) küçük bir değişiklikle nasıl değiştiğini anlamaya çalışır. Saldırı, modelin karar sınırına yakın girdiler üzerinde uygulanarak, sınıf tahmininin değiştirilmesini sağlar.

\newpage

\subsection{Jacobian Saliency Map Attack (JSMA)} 

JSMA, modelin girdiye verdiği yanıtları analiz ederek hangi giriş özelliklerinin hedeflenen sınıfa doğru değiştirileceğini seçmek için "Jacobian Matris" kullanılarak oluşturulan bir saliency map (önem haritası) kullanır. 

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.estimators.classification import TensorFlowV2Classifier
from art.attacks.evasion import SaliencyMapMethod

art_model = TensorFlowV2Classifier(model=model, nb_classes=10, 
                                  input_shape=(32, 32, 3), clip_values=(0, 1))
jsma = SaliencyMapMethod(classifier=art_model, theta=1, gamma=0.1)
adv_test = jsma.generate(x=x_test[:16])
\end{lstlisting}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{images/jsma_results.png}
    \caption{}
\end{figure}

\newpage

\subsection{Shadow Attack}

Shadow Attack, modelin verdiği tahminlerin doğruluğunu etkilemek için girdiye küçük perturbasyonlar (bozulmalar) ekler. Bu perturbasyonlar genellikle giriş verisinde fark edilmesi zor olacak şekilde tasarlanır. Modelin çıkışlarına karşı duyarlılık analizi yaparak, hangi giriş özelliklerinin modelin kararını en çok etkilediği belirlenir ve bu özellikler hedeflenerek manipüle edilir.

\newpage

\subsection{Shape Shifter Attack}

Shape Shifter Attack, bir modelin doğru sınıflandırdığı bir görüntüyü, fark edilemeyecek kadar küçük değişikliklerle manipüle edip modelin yanlış bir sınıflandırma yapmasına neden olur.

\newpage

\subsection{Sign-OPT Attack}

Sign-OPT Attack, adversarial örneklerin optimize edilmesini sağlar. Saldırı, zeroth-order optimization yöntemini kullanarak bir hedef sınıfa ulaşmaya çalışır. Saldırının temel mantığı, modelin karar sınırlarını keşfederek küçük, insan gözüyle fark edilemeyecek değişiklikler yapmaktır. Bu değişiklikler, modelin tahmin sonuçlarını değiştirecek şekilde optimize edilir.

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.estimators.classification import TensorFlowV2Classifier
from art.attacks.evasion import SignOPTAttack

art_model = TensorFlowV2Classifier(model=model, nb_classes=10, 
                                   input_shape=(32, 32, 3), clip_values=(0, 1))
sign_opt_attack = SignOPTAttack(estimator=art_model, targeted=False, max_iter=100, query_limit=2000)
adv_test = sign_opt_attack.generate(x=x_test[:16])
\end{lstlisting}

\newpage

\subsection{Spatial Transformations Attack}

Spatial Transformation Attack, modelin giriş verisinde uzaysal (spatial) dönüşümler yaparak modelin yanlış sınıflandırma yapmasını sağlar. Amacı, modelin hassas olduğu küçük geometrik dönüşümlerle, modelin yanlış sınıflandırma yapmasına neden olmaktır. Uzaysal dönüşümler, giriş verisinin piksel konumlarını veya yapısını değiştirir. Bu dönüşümler, küçük açısal rotasyonlar, kaydırmalar, ölçekleme veya perspektif değişimler olabilir.

\begin{itemize}
    \item \textbf{Rotation}: Görüntüde küçük açısal döndürmeler yapılır.
    \item \textbf{Translation}: Görüntüde yatay veya dikey kaydırmalar yapılır.
    \item \textbf{Scaling}: Görüntünün boyutları küçültülür veya büyültülür.
    \item \textbf{Shearing}: Görüntüye paralel kaydırmalar uygulanır.
    \item \textbf{Perspective Transform}: Görüntünün perspektifinde değişiklikler yapılır.
\end{itemize}

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.estimators.classification import TensorFlowV2Classifier
from art.attacks.evasion import SpatialTransformation

art_model = TensorFlowV2Classifier(
    model=model, nb_classes=10, 
    input_shape=(32, 32, 3), clip_values=(0, 1)
    )
st = SpatialTransformation(
    classifier=art_model, 
    max_translation=15.0, 
    max_rotation=25.0)
adv_test = st.generate(x=x_test[:16], y=y_test[:16])
\end{lstlisting}

\newpage

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{images/spatial_transformation_results.png}
    \caption{}
\end{figure}

\newpage

\subsection{Square Attack}

Adını, giriş verisindeki belirli kare şeklindeki bölgelere yapılan manipülasyonlardan alır. Square Attack, görüntü üzerinde rastgele kare şeklinde pikselleri değiştirerek çalışır. Bu değişiklikler, modelin kararını etkileyerek yanlış sınıflandırmaya yol açabilir. Saldırı, özellikle verinin sınıflandırma sınırlarına yakın yerlerde yapılır, bu da modelin sınıflandırma doğruluğunu bozmaya yönelik etkili bir strateji sunar.

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.estimators.classification import TensorFlowV2Classifier
from art.attacks.evasion import SquareAttack

art_model = TensorFlowV2Classifier(model=model, nb_classes=10, 
                                   input_shape=(32, 32, 3), clip_values=(0, 1))
sa = SquareAttack(estimator=art_model, norm=np.inf, max_iter=100)
adv_test = sa.generate(x=x_test[:16])
\end{lstlisting}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/square_attack_results.png}
    \caption{}
\end{figure}

\newpage

\subsection{Targeted Universal Perturbation Attack (T-UAP)}

T-UAP, modele evrensel bir pertürbasyon ekleyerek hedef sınıfa yönelik yanlış sınıflandırma yapılmasını sağlar. Evrensel olarak nitelendirilmesi, bir modelin üzerine eğitildiği veri setinin büyük bir kısmını etkileyebilecek tek bir pertürbasyonun bulunması anlamına gelir. Pertürbasyon, her veri noktasına eklendiğinde modelin çıkışını hedef sınıfa yönlendiren bir yapıya sahiptir.

\newpage

\subsection{Universal Perturbation Attack (UAP)}

Universal Perturbation Attack, her bir örneğe küçük bir pertürbasyon ekleyerek modelin tahminlerini yanlış yapmaya çalışır. Pertürbasyon, evrensel bir bozucu sinyal olacak şekilde optimize edilir, yani tüm veri seti üzerinde işe yarayacak şekilde ayarlanır. Model, bu pertürbasyon uygulandığında hatalı sınıflandırmalar yapar.

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.attacks.evasion import UniversalPerturbation

attack = UniversalPerturbation(
    classifier=art_model, 
    attacker="fgsm", 
    max_iter=5
)
adv_test = attack.generate(x=x_test)
\end{lstlisting}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{images/universal_perturbation_example.png}
    \caption{}
\end{figure}

\newpage

\subsection{Virtual Adversarial Method (VAM)}

VAM, herhangi bir sınıflandırma etiketi kullanmadan girişlere adversarial perturbasyonlar ekler, bu yüzden "virtual (sanal)" olarak adlandırılır. VAM, denetimli olmayan adversarial örnekler oluşturarak modelin güvenliğini test eder ve sınıflandırıcının karar sınırlarını zorlar. Yarı denetimli öğrenme yöntemlerinde kullanılır.

\newpage

\subsection{Wasserstein Attack}

Wasserstein Attack, giriş verilerine minimal değişiklikler yaparak modelin tahminlerini yanıltmaya çalışır. Bu saldırı, bozucu perturbasyonların hem görünmez hem de model için anlamlı bir şekilde etkili olmasını hedefler. Bu amaçla Wasserstein mesafesi temel alınır. Amaç, giriş verilerinin orijinal dağılımından çok uzaklaşmadan modelin tahminlerini değiştiren perturbasyonlar üretmektir.

Wasserstein Attack, Wasserstein mesafesi kullanarak adversarial örnekler oluşturur. Bu mesafe, iki olasılık dağılımı arasındaki minimum taşıma maliyetini ölçer.

\[ W(p, q) = \inf_{\gamma \in \Gamma(p, q)} \mathbb{E}_{(x, y) \sim \gamma}[\| x - y \|] \]

Burada:

\begin{itemize}
    \item $p$ ve $q$: Giriş veri setinin ve bozucu pertürbasyonun olasılık dağılımları.
    \item $\Gamma(p, q)$: $p$ ve $q$ arasında ortak dağılımların kümesi.
    \item $x$ ve $y$: Girş verilerinin ve bozucu pertürbasyonların bireysel örnekleri.
    \item $\| x - y \|$ $x$ ve $y$ arasındaki mesafe.
\end{itemize}

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.estimators.classification import TensorFlowV2Classifier
from art.attacks.evasion import Wasserstein

art_model = TensorFlowV2Classifier(
    model=model,
    loss_object=CategoricalCrossentropy(from_logits=True), 
    nb_classes=10, input_shape=(32, 32, 3), clip_values=(0, 1))
wa = Wasserstein(estimator=art_model, targeted=False, eps=0.7)
adv_test = wa.generate(x=x_test[:4])
\end{lstlisting}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{images/wasserstein_attack_results.png}
    \caption{}
\end{figure}

\newpage

\subsection{Zeroth-Order Optimization (ZOO) Attack}

ZOO Attack, modelin parametrelerine veya gradyan bilgilerine ihtiyaç duymadan, doğrudan modele yapılan sorgularla saldırıyı gerçekleştirir. ZOO Attack, zeroth-order optimization (sıfırıncı derece optimizasyon) adı verilen bir yöntem kullanır. Bu yöntem, gradyan bilgisine erişmeden, sadece modelin çıktısına bakarak saldırı yapılabilmesini sağlar. Gradyanın yaklaşık bir değeri hesaplanarak optimizasyon yapılır, bu da saldırının amacına ulaşmak için yeterlidir.

ZOO Attack, gradyanı doğrudan hesaplamadığından dolayı finite-difference yöntemini kullanır. Bu yöntemde gradyan tahmini aşağıdaki gibi yapılır:

\[ \frac{\partial J(x)}{\partial x_i} \approx \frac{J(x + h e_i) - J(x - h e_i)}{2h} \]

Burada:

\begin{itemize}
    \item $J(x)$: Girdi $x$ için modelin kayıp fonksiyonu.
    \item $h$: Küçük bir adım boyutu.
    \item $e_i$: Girdi uzayındaki standart temel vektör.
    \item $\frac{\partial J(x)}{\partial x_i}$: Girdi $x$'in, $i$. bileşeni ile ilgili yaklaşık gradyan.
\end{itemize}

\subsubsection{Python Kodu}

\begin{lstlisting}[language=Python]
from art.estimators.classification import TensorFlowV2Classifier
from art.attacks.evasion import ZooAttack

art_model = TensorFlowV2Classifier(
    model=model, 
    loss_object=CategoricalCrossentropy(from_logits=True),
    nb_classes=10, input_shape=(32, 32, 3), clip_values=(0, 1))
zoo = ZooAttack(classifier=art_model, confidence=0.3, targeted=False, learning_rate=0.1)
adv_test = zoo.generate(x=x_test[:16])
\end{lstlisting}

\newpage

\subsection{Simultaneous Perturbation Stochastic Approximation (SPSA) Attack}

SPSA, gradient-free optimizasyon kullanarak adversarial pertürbasyonlar oluşturur. SPSA, gradient-free bir saldırı olarak, modelin gradyan bilgisine ihtiyaç duymadan adversarial örnekler oluşturur. Bu özelliği nedeniyle black-box bir saldırı türüdür. Yani, modelin çıktısını bozmak için girdiler eklenen küçük değişikliklerin etkisini tahmin etmek amacıyla, girişin yalnızca birkaç farklı versiyonu test edilir. Bu işlem iki adımda yapılır:

\begin{itemize}
    \item \textbf{Simultaneous Perturbation}: Girdi örneklerinin küçük rastgele değişiklikler ile bozulması.
    \item \textbf{Stochastic Approximation}: Simültane pertürbasyonların etkilerini izleyerek gradientin yaklaşık değerini hesaplama. Bu değer, modele erişmeden elde edilen bir tahmindir.
\end{itemize}

Bu adımlarda modelin çıktısına etki eden değişikliklerin yönü hesaplanarak, adversarial bir saldırı oluşturulur.

\subsubsection{Çalışma Adımları}

\begin{enumerate}
    \item Rastgele bir başlangıç pertürbasyonu oluşturulur.
    \item Girdi verisine aynı anda birçok küçük pertürbasyon eklenir. Bu pertürbasyonlar rastgele oluşturulur.
    \item Her pertürbasyonlu giriş için modelin çıktıları hesaplanır. 
    \item Gradyanın tam hesaplanmasına gerek olmadan, pertürbasyonların model çıktısındaki etkisi kullanılarak, gradyanın yaklaşık değeri bulunur.
    \item Bu yaklaşık gradyan bilgisi kullanılarak giriş üzerinde uygulanacak en etkili adversarial pertürbasyon belirlenir.
    \item Bu işlem belirli bir iterasyon boyunca tekrarlanır.
\end{enumerate}

\newpage

\subsection{Additive Noise Attack}

Gürültü eklenerek modeiln yanlış tahminler yapması hedeflenir. Hem white-box hem de black-box saldırılarda uygulanabilir. Additive Noise Attack, modelin girişine küçük ama stratejik gürültüler ekleyerek çalışır. Bu gürültüler, giriş uzayında rastgele veya belirli bir dağılıma uygun olarak seçilen pertürbasyonlardan oluşur.

\newpage

\subsection{Binarization Refinement Attack}

Binarization Refinement Attack, görüntülerdeki piksel değerlerini belirli bir eşikleme işlemi ile modelin yanlış tahminler yapmasını hedefler. Görüntülerde piksel değerleri 0-255 aralığında değişir, ancak binarizasyon işlemi bu değerleri sadece iki farklı seviyeye indirger. Bu saldırı, binarizasyonun sonucunu manipüle ederek, modelin bu düşük çözünürlüklü veya iki seviyeli görüntülerle yanlış tahminler yapmasını sağlar. 

\subsubsection{Çalışma Adımları}

\begin{enumerate}
    \item Girdi, belirli bir eşikleme işlemi ile binarize edilir. Piksellerin belirli bir değerin üzerinde olanları 1, altındaki olanlar ise 0 değerine indirilir.
    \item Giriş verisine eklenen küçük bir pertürbasyon eklenir ve binarizasyon işlemi tekrarlanır. Amaç, bu küçük pertürbasyonun binarize edilmiş görüntüyü etkileyerek modelin çıktısını değiştirmesini sağlamaktır.
    \item Binarize edilmiş pertürbasyonlu giriş, model tarafından işlenir. Modelin çıktısının nasıl değiştiği gözlemlenir.
\end{enumerate}

\newpage

\subsection{Blended Noise Attack}

Blended Noise Attack, modelin girişine eklenen karışık gürültü ile modeiln yanlış tahmin yapmasını hedefler. Diğer saldırılardan farkı, gürültünün hedef giriş ile rastgele bir referans giriş arasında harmanlanmış olmasıdır. Amaç, girdi verisinde minimum düzeyde değişiklik yaparken, modelin davranışını ciddi şekilde etkilemektir. Bu sayede saldırı, giriş verisini orijinal veriden çok uzaklaştırmadan bir değişiklik yaparak modeli yanıltmayı hedefler. 

\[ x' = \alpha \cdot x + (1 - \alpha) \cdot r \]

Burada:

\begin{itemize}
    \item $x$ Girdi verisi.
    \item $r$: Referans verisi.
    \item $\alpha$: Karışım oranı.
\end{itemize}

\newpage

\subsection{Gaussian Blur Attack}

Gaussian Blur Attack, bir görüntüye Gaussian bulanıklaştırma uygulayarak modelin kararlarını yanlış yönlendirmeyi hedefler. Bir görüntüdeki piksellerin etrafındaki komşu piksellerin ortalaması alınarak yumuşatılması işlemine dayanır. Bu yumuşatma işlemi, görüntünün detaylarını kaybetmesine neden olur. Gaussian blur filtresi, her pikselin komşularıyla birlikte bir Gaussian dağılımı ile ağırlıklandırılmasıyla hesaplanır.

\[ G(x, \sigma) = \frac{1}{2\pi\sigma^2} \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right) \]

Burada:

\begin{itemize}
    \item $\sigma$: Standart sapma, bulanıklığın derecesini ifade eder.
    \item $x$ ve $y$: Pikselin komşu pikseller ile arasındaki mesafeyi ifade eden koordinatlardır.
    \item $\exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right)$: Pikselin komşuları ile olan uzaklığını göre ağırlıklandırılmasını sağlar.
\end{itemize}

Gaussian blur, pikselin komşularına olan uzaklık arttıkça o pikselin yeni değerine daha az katkıda bulunur.

\newpage

\subsection{L2 Contrast Reduction Attack}

L2 Contrast Reduction Attack, hedef bir görüntünün kontrastını azaltarak, görüntüyü daha "düz" hale getirir ve böylece modelin görüntüdeki ayrıntıları algılama kapasitesini zayıflatır. Bu işlem, her bir pikselin parlaklığının ortalama bir değere yaklaştırılması ile gerçekleştirilir. Saldırıda, L2 normuna göre en uygun kontrast azaltımı hesaplanır.

Görüntüdeki her bir pikselin ortalama parlaklık değeri $\mu$ hesaplanır. Bu ortalama değer, tüm piksellerin yeni parlaklık değerlerine yaklaşacağı bir referans noktasıdır.

\[ \mu = \frac{1}{n} \sum_{i=1}^{n} x_i \]

Görüntüdeki her pikselin değeri ortalama değere yaklaştırılarak kontrast azaltılır. L2 Normu kullanılarak piksellerin bu yeni değeri hesaplanır.

\[ x' = x - \epsilon \cdot (x - \mu) \]

Burada $\epsilon$, kontrastın ne kadar azaltılacağını kontrol eden bir parametredir.

\newpage

\subsection{Fast Minimum Norm Adversarial Attack}

FMN saldırısı, genellikle L2 veya L-$\infty$ normlarına dayanarak çalışır. Saldırının ana fikri, bir girdinin adversarial bir örneğe dönüştürülmesi sırasında girdiye minimum normda bir pertürbasyon eklemektir. Pertürbasyonun boyutunu optimize ederek saldırının hem hızlı hem de minimum normlu olmasını sağlamak, bu saldırının temel hedefidir.

\newpage

\subsection{Gen Attack}

Gen Attack, genetik algoritmaların evrimsel süreçlerine dayanan bir saldırı türü olup, yapay sinir ağlarına yönelik saldırılar için optimize edilmiştir. Bu saldırı, evrimsel algoritmaların seçilim, çaprazlama ve mutasyon adımlarını kullanarak yapay zeka modellerine zarar verebilecek minimum değişiklikleri bulmayı hedefler.

\subsubsection{Çalışma Adımları}

\begin{enumerate}
    \item Bir başlangıç popülasyonu belirlenir. Bu popülasyon, çeşitli küçük pertürbasyonlar içeren aday örneklerden oluşur.
    \item Her bir birey bir fitness fonksiyonu ile değerlendirilir. Fitness fonksiyonu, bir bireyin adversarial örnek olma potansiyelini ölçer.
    \item Daha yüksek fitness değerine sahip olan bireyler, sonraki nesillere geçebilmek için seçilir.
    \item Seçilen bireyler, yeni bireyler oluşturmak için çaprazlanır. Bu işlem, iki ebeveynin genetik materyalini karıştırarak yeni bir adversarial örnek adayı oluşturur.
    \item Çaprazlama sonucunda oluşan yeni bireylere, küçük rastgele değişiklikler (mutasyon) eklenir. Mutasyon, yeni ve daha etkili adversarial örneklerin bulunmasını sağlar.
    \item Bu süreç belirli bir iterasyon boyunca tekrarlanır.
\end{enumerate}

\newpage

\subsection{Momentum Iterative Fast Gradient Sign Method (MI-FGSM)}

MI-FGSM, klasik FGSM saldırısının geliştirilmiş bir versiyonudur ve FGSM'nin temel sınırlamalarını aşmak için momentum kavramını kullanır. Momentum, optimizasyon problemlerinde öğrenme hızını artırmak ve sıkışmalardan kaçınmak için kullanılır ve bu saldırı yönteminde de adversarial saldırının başarısını artırmak için kullanılır. MI-FGSM, klasik FGSM gibi modelin gradyan bilgisine dayalıdır ancak her iterasyonda momentum terimi ekleyerek saldırının daha etkili olmasını sağlar. Momentum terimi, gradyan bilgisinin birikmesini sağlayarak saldırının daha istikrarlı ve tutarlı olmasını sağlar. Ayrıca, savunma mekanizmalarına karşı dayanıklılığı artırır, çünkü momentum, modelin gradyan bilgisinin doğrusal olmayan yollarını daha iyi keşfetmesini sağlar.

\subsubsection{Çalışma Adımları}

\begin{enumerate}
    \item Modelin girdisi üzerinde gradyan hesaplanır ve bu gradyana göre bir pertürbasyon eklenir.
    \item Her iterasyonda, önceki iterasyonlardan gelen gradyanlar momentum terimi olarak biriktirilir. Bu, her iterasyondaki gradyanın doğrudan kullanılmasından farklıdır. Momentum, önceki gradyanların bir kısmını hatırlayarak yeni gradyanla birleştirilir. Bu sayede saldırının yönü daha iyi optimize edilir.
    \item Bu adımda momentumlu gradyan işaretine göre giriş güncellenir. Bu işlem birçok iterasyon boyunca devam eder.
    \item Girdi, L-$\infty$ normu içerisinde sınırlandırılır. Bu, bozulmaların aşırıya kaçmamasını sağlar, yani girdi üzerindeki değişikliklerin insan gözüyle fark edilemeyecek kadar küçük olmasını hedefler.
\end{enumerate}

\newpage

\subsection{Pointwise Attack}

Pointwise Attack, iteratif bir saldırı yöntemidir ve her adımda saldırının başarısını artıracak şekilde tek bir pikselde veya birkaç pikselde değişiklik yapar. Bu saldırı, hedef modelin sınıflandırmasında yanlış sonuç üretmeye odaklanır, ancak bunu yaparken saldırının minimum seviyede fark edilmesini sağlar.

\newpage

\subsection{Sparse L1 Descent Attack}

Sparse L1 Descent Attack, seyrek (sparse) değişikliklerle modelin doğruluğunu bozmayı hedefler. Yani, girdi üzerinde çok az değişiklik yaparak, modelin kararını değiştirmeye çalışır. Bu sayede, saldırı hem etkili olur hem de saldırının fark edilmesi zorlaşır. L1 normu kullandığı için, minimum sayıda özelliği (veya pikseli) değiştirerek modeli yanıltmayı amaçlar. L1 normu, değişikliklerin toplam mutlak büyüklüğünü ölçmekte kullanılır, bu da giriş vektörünün kaç elemanının değiştirildiğini kontrol etmeye yardımcı olur. İteratif optimizasyon teknikleri kullanılarak çalışır ve her iterasyonda, modelin en hassas olduğu bölgeler üzerine odaklanılır.

\newpage

\subsection{Virtual Adversarial Attack}

Virtual Adversarial Attack, modelin kararlılığını ölçmek ve onu daha güvenli hale getirmek amacıyla kullanılır. Denetimsiz veya yarı denetimli öğrenmede, modelin karar sınırlarını anlamaya çalışır ve bu sınırların çevresinde küçük değişiklikler yaparak modelin kararsız noktalarını bulur. Saldırının amacı, modelin aynı veri noktası için verdiği çıktının çok farklı olmamasını sağlamaktır. Bu saldırı sırasında kullanılan pertürbasyonlar, sanal olarak (virtual) modellenir, çünkü bu süreçte verinin etiketlerine ihtiyaç yoktur.

\newpage

\subsection{Free Range Attack}

Free Range Attack, modelin girdisini geniş bir özgürlük alanında manipüle eder ve hedeflenen bir çıktıya ulaşmak için çeşitli değişiklikler yapar. Bu saldırılar sırasında herhangi bir norm sınırlaması olmadan geniş çapta pertürbasyonlar yapılabilir. Girdinin içeriği, herhangi bir boyutta veya şekilde manipüle edilerek modelin çıktısı ciddi şekilde bozulmaya çalışılır.

\newpage

\subsection{Restratined Attack}

Restratined Attack, modelin sınıflandırma veya tahmin doğruluğunu bozmak amacıyla, uygulanan pertürbasyonların belirli kısıtlamalar altında gerçekleştirildiği bir saldırı türüdür. Yani, girdiye eklenen manipülasyonlar veya pertürbasyonlar belirli bir norm, boyut, veya aralıkla sınırlandırılır. Bu yöntem, pertürbasyonun algılanabilirliğini en aza indirmek veya sistem tarafından tespit edilmemesini sağlamak için kullanılır.

\newpage

\subsection{Binary Greedy}

Karar tabanlı bir saldırıdır. Bu yöntem, adversarial saldırı uygularken, ikili (binary) bir karar alma stratejisi kullanır. Amaç, her adımda, hedef modele karşı en fazla zararı verecek ikili seçimleri yapmak ve modelin çıktısını olumsuz yönde etkilemektir. Her bir değişiklik, girdiyi değiştirip değiştirmemek gibi bir karar almayı içerir. Her iterasyonda, girdinin belirli bölümlerini değiştirmeye veya değiştirmemeye karar verilir. Modelin çıktısını en fazla bozacak değişiklik yapılır. Eğer değişiklikler modelin kararını olumsuz etkiliyorsa, bu değişiklikler korunur. Bu süreç, modelin çıktısı ciddi şekilde bozulana kadar tekrarlanır.

\newpage

\subsection{Coordinate Greedy}

Karar tabanlı bir saldırıdır. Coordinate Greedy, her seferinde girdideki yalnızca tek bir boyutta (koordinatta) değişiklik yaparak, modele karşı saldırı gerçekleştirir. Bu yöntem, daha ince ayarlı ve kontrollü bir saldırı gerçekleştirir çünkü yalnızca belirli bir yönü veya girdinin bir kısmını hedef alır. Her bir adımda, belirli bir koordinattaki değerin optimize edilip edilmemesine karar verilir. Her iterasyonda, girdinin belirli bir koordinatı seçilir. Seçilen koordinattaki değeri en fazla optimize eden değişiklik seçilir. Bu işlem, her adımda yalnızca bir koordinatta değişiklik yapılarak devam eder.

\newpage