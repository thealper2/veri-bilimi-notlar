\section{Attention Türleri}
\subsection{Luong Attention (Global Attention)}
Makine çevirisi modellerinde yaygın olarak kullanılır. Global Attention olarak da bilinir. Luong Attention'da, encoder'in her bir gizli durumu  $(h_j)$ ve decoder'in şu anki gizli durumu $(s_j)$ arasındaki benzerliği hesaplamak için bir skor fonksiyonu kullanılır. Luong Attention'da üç farklı skor fonksiyonu kullanılabilir:
\begin{itemize}
	\item \textbf{Nokta Çarpımı (Dot Product)}: $score(h_j, s_t) = h \frac{\text{T}}{\text{j}} * s_t $
	\item \textbf{Genel (General)}: $score(h_j, s_t) = h \frac{\text{T}}{\text{j}} * W_a * s_t$
	\item \textbf{Birleştirme (Concatenate)}: $score(h_j, s_t) = v \frac{\text{T}}{\text{a}} * tanh(W_a [j_j;s_t])$
\end{itemize} 

Elde edilen değerler softmax ile normalize edilir ve dikkatin hangi encoder durumlarına odaklanacağını belirleyen bir dikkat dağılım (attention distribution) elde edilir. Encoder'in tüm gizli durumları, dikkat dağılımı ağırlıklarına göre ağırlıklı ortalaması alınarak bir bağlam vektörü (context vector) oluşturulur. Decoder'in şu anki gizli duru ve bağlam vektörü birleştirilerek yeni bir dikkatli durum (attentive state) hesaplanır.

\subsubsection{Python Kodu}

TensorFlow ile Luong Attention implementasyonu örneği aşağıda verilmiştir.

\begin{lstlisting}[language=Python]
class LuongAttention(layers.Layer):
    def __init__(self):
        super().__init__()

    def call(self, decoder_state_h, decoder_state_c, encoder_output):
        decoder_state = tf.add(decoder_state_h, decoder_state_c)[:, :, tf.newaxis]
        score = layers.dot([encoder_output, decoder_state], axes=[2, 1])
        attention_weights = tf.nn.softmax(score, axis=1)
        context_vector = tf.reduce_sum(attention_weights * encoder_output, axis=1)
        return context_vector, attention_weights

attention_layer = LuongAttention()
decoder_state_h, decoder_state_c = encoder_state_h, encoder_state_c
\end{lstlisting}

\subsection{Bahdanau Attention (Additive Attention)}
Seq2seq modellerinde yaygın olarak kullanılır. Additive Attention olarak da bilinir. Luong'dan farkı skor hesaplamasında ek bir ağırlık matrisi kullanılır. Bahdanau Attention, skor hesaplamasını bir sinir ağı katmanı ile yapar. Elde edilen değerler softmax ile normalize edilir ve dikkatin hangi encoder durumlarına odaklanacağını belirleyen bir dikkat dağılım (attention distribution) elde edilir. Encoder'in tüm gizli durumları, dikkat dağılımı ağırlıklarına göre ağırlıklı ortalaması alınarak bir bağlam vektörü (context vector) oluşturulur. Decoder'in şu anki gizli duru ve bağlam vektörü birleştirilerek yeni bir dikkatli durum (attentive state) hesaplanır.

\subsubsection{Python Kodu}

TensorFlow ile Bahdanau Attention implementasyonu örneği aşağıda verilmiştir.

\begin{lstlisting}[language=Python]
class BahdanauAttention(layers.Layer):
    def __init__(self, units):
        super(BahdanauAttention, self).__init__()
        self.W1 = layers.Dense(units)
        self.W2 = layers.Dense(units)
        self.V = layers.Dense(1)

    def call(self, query, values):
        query_with_time_axis = tf.expand_dims(query, 1)
        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))
        attention_weights = tf.nn.softmax(score, axis=1)
        context_vector = attention_weights * values
        context_vector = tf.reduce_sum(context_vector, axis=1)
        return context_vector, attention_weights

attention_layer = BahdanauAttention(10)
attention_result, attention_weights = attention_layer(encoder_state, encoder_output)

print("Bahdanau Attention result shape:", attention_result.shape)
print("Bahdanau Attention weights shape:", attention_weights.shape)
\end{lstlisting}

\subsection{Self-Attention}
Transformer modellerinin temel bileşenidir. Bir metin içindeki tüm öğelerin birbirine dikkat etmesini sağlar. Paralel hesaplama imkanı sağlar. Girdi dizisi sorgu (query), key(anahtar) ve değer vektörleri (value) olmask üzere üç farklı vektöre dönüştürülür. Sorgu ve anahtar vektörlerinin nokta çarpımı (dot product) ile skorlar hesaplanır ve ölçeklendirilir, elde edilen değerler softmax ile normalize edilir. Ağırlık ortalama alınarak yeni bir vektör elde edilir.

\subsection{Multi-Head Attention}
Birden fazla self-attention mekanizmasını paralel olarak kullanır ve çıktıları birleştirir. Girdi vektörleri, farklı ağırlık matriksleri ile farklı başlıklara (heads) bölünür. Her başlık için bağımsız self-attention uygulanır. Tüm başlıklardan elde edilen çıktılar birleştirilir ve son bir ağırlık matrisi ile yeniden dönüştürülür.

\subsection{Hierarchical Attention}
Çok katmanlı yapıların farklı seviyelerinde dikkat mekanizmalarını kullanır. Genellikle doküman seviyesi ve cümle seviyesi gibi hiyerarşik yapılarda kullanılır.

\begin{itemize}
	\item \textbf{Cümle Seviyesi}: İlk olarak cümle içindeki kelimeler arasında dikkat hesaplanır ve cümle temsilleri oluşturulur.
	\item \textbf{Doküman Seviyes}: Ardından, cümle temsilleri arasında dikkat hesaplanarak doküman temsili elde edilir.
\end{itemize}

\subsubsection{Python Kodu}

TensorFlow kütüphanesi ile "Hierarchical Attention Network" implementasyonu yapılmıştır.

\begin{lstlisting}[language=Python]
class AttentionWithContext(layers.Layer):
    def __init__(self,
                 W_regularizer=None, u_regularizer=None, b_regularizer=None,
                 W_constraint=None, u_constraint=None, b_constraint=None,
                 bias=True, **kwargs):
        self.init = initializers.get('glorot_uniform')

        self.W_regularizer = regularizers.get(W_regularizer)
        self.u_regularizer = regularizers.get(u_regularizer)
        self.b_regularizer = regularizers.get(b_regularizer)

        self.W_constraint = constraints.get(W_constraint)
        self.u_constraint = constraints.get(u_constraint)
        self.b_constraint = constraints.get(b_constraint)

        self.bias = bias
        super(AttentionWithContext, self).__init__(**kwargs)

    def build(self, input_shape):
        assert len(input_shape) == 3

        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1]),
                                 initializer=self.init,
                                 name='{}_W'.format(self.name),
                                 regularizer=self.W_regularizer,
                                 constraint=self.W_constraint)
        if self.bias:
            self.b = self.add_weight(shape=(input_shape[-1],),
                                     initializer='zero',
                                     name='{}_b'.format(self.name),
                                     regularizer=self.b_regularizer,
                                     constraint=self.b_constraint)

        self.u = self.add_weight(shape=(input_shape[-1], 1),
                                 initializer=self.init,
                                 name='{}_u'.format(self.name),
                                 regularizer=self.u_regularizer,
                                 constraint=self.u_constraint)

        super(AttentionWithContext, self).build(input_shape)

    def call(self, x, mask=None):
        uit = K.dot(x, self.W)

        if self.bias:
            uit += self.b
            
        uit = K.tanh(uit)
        ait = K.squeeze(K.dot(uit, self.u), -1)
        a = K.exp(ait)
        if mask is not None:
            a *= K.cast(mask, K.floatx())

        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())
        a = K.expand_dims(a)
        weighted_input = x * a
        return K.sum(weighted_input, axis=1)

    def compute_output_shape(self, input_shape):
        return input_shape[0], input_shape[-1]

word_input = layers.Input(shape=(max_sentence_len,), dtype='float32')
x = layers.Embedding(len(tokenizer.word_index) + 1, 
                     embed_size, 
                     weights=[embedding_matrix], 
                     input_length=max_sentence_len, 
                     trainable=False)(word_input)
x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, kernel_regularizer="l2"))(x)
x = layers.TimeDistributed(layers.Dense(256, kernel_regularizer="l2"))(x)
x = AttentionWithContext()(x)
word_encoder = models.Model(word_input, x)

sentence_input = layers.Input(shape=(max_sentence_num, max_sentence_len), dtype='float32')
x = layers.TimeDistributed(word_encoder)(sentence_input)
x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, kernel_regularizer="l2"))(x)
x = layers.TimeDistributed(layers.Dense(256, kernel_regularizer="l2"))(x)
x = layers.Dropout(0.5)(AttentionWithContext()(x))
output_layer = layers.Dense(authors.nunique(), activation='softmax')(x)

model = models.Model(sentence_input, output_layer)
model.compile(loss='categorical_crossentropy', 
              optimizer='adam', 
              metrics=['accuracy'])
\end{lstlisting}

\subsection{Monotonic Attention}
Sıralı veriler ile çalışırken dikkatin yalnızca belirli bir yönü takip etmesini sağlar. Bu, dikkatin sıralı bir şekilde ilerlemesini zorunlu kılar. Dikkat ağırlıkları, sıralı bir şekilde ilerleyerek hesaplanır. Bir konumdan bir sonraki geçişler sıralı olur. Bağlam vektörü, diğerlerine benzer şekilde hesaplanır ancak dikkatin sıralılığı korunur.

\subsection{Luong Monotic Attention}
Genellikle konuşma tanıma problemlerinde kullanılır. Luong Attention mekanizmasının sıralı veriler ile çalışacak şekilde düzenlenmiş halidir. Dikkat mekanizması zaman ilerledikçe sıralı ve monoton bir şekilde ilerler.

\subsection{Bahdanau Monotic Attention}
Bahdanau Attention mekanizmasının sıralı veriler ile çalışacak şekilde düzenlenmiş halidir. Dikkat mekanizması zaman ilerledikçe sıralı ve monoton bir şekilde ilerler.

\newpage