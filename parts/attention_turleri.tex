\section{Attention Türleri}
\subsection{Luong Attention (Global Attention)}
Makine çevirisi modellerinde yaygın olarak kullanılır. Global Attention olarak da bilinir. Luong Attention'da, encoder'in her bir gizli durumu  $(h_j)$ ve decoder'in şu anki gizli durumu $(s_j)$ arasındaki benzerliği hesaplamak için bir skor fonksiyonu kullanılır. Luong Attention'da üç farklı skor fonksiyonu kullanılabilir:
\begin{itemize}
	\item \textbf{Nokta Çarpımı (Dot Product)}: $score(h_j, s_t) = h \frac{\text{T}}{\text{j}} * s_t $
	\item \textbf{Genel (General)}: $score(h_j, s_t) = h \frac{\text{T}}{\text{j}} * W_a * s_t$
	\item \textbf{Birleştirme (Concatenate)}: $score(h_j, s_t) = v \frac{\text{T}}{\text{a}} * tanh(W_a [j_j;s_t])$
\end{itemize} 

Elde edilen değerler softmax ile normalize edilir ve dikkatin hangi encoder durumlarına odaklanacağını belirleyen bir dikkat dağılım (attention distribution) elde edilir. Encoder'in tüm gizli durumları, dikkat dağılımı ağırlıklarına göre ağırlıklı ortalaması alınarak bir bağlam vektörü (context vector) oluşturulur. Decoder'in şu anki gizli duru ve bağlam vektörü birleştirilerek yeni bir dikkatli durum (attentive state) hesaplanır.

\subsubsection{Python Kodu}

TensorFlow ile Luong Attention implementasyonu örneği aşağıda verilmiştir.

\begin{lstlisting}[language=Python]
class LuongAttention(layers.Layer):
    def __init__(self):
        super().__init__()

    def call(self, decoder_state_h, decoder_state_c, encoder_output):
        decoder_state = tf.add(decoder_state_h, decoder_state_c)[:, :, tf.newaxis]
        score = layers.dot([encoder_output, decoder_state], axes=[2, 1])
        attention_weights = tf.nn.softmax(score, axis=1)
        context_vector = tf.reduce_sum(attention_weights * encoder_output, axis=1)
        return context_vector, attention_weights

attention_layer = LuongAttention()
decoder_state_h, decoder_state_c = encoder_state_h, encoder_state_c
\end{lstlisting}

\newpage

\subsection{Bahdanau Attention (Additive Attention)}
Seq2seq modellerinde yaygın olarak kullanılır. Additive Attention olarak da bilinir. Luong'dan farkı skor hesaplamasında ek bir ağırlık matrisi kullanılır. Bahdanau Attention, skor hesaplamasını bir sinir ağı katmanı ile yapar. Elde edilen değerler softmax ile normalize edilir ve dikkatin hangi encoder durumlarına odaklanacağını belirleyen bir dikkat dağılım (attention distribution) elde edilir. Encoder'in tüm gizli durumları, dikkat dağılımı ağırlıklarına göre ağırlıklı ortalaması alınarak bir bağlam vektörü (context vector) oluşturulur. Decoder'in şu anki gizli duru ve bağlam vektörü birleştirilerek yeni bir dikkatli durum (attentive state) hesaplanır.

\subsubsection{Python Kodu}

TensorFlow ile Bahdanau Attention implementasyonu örneği aşağıda verilmiştir.

\begin{lstlisting}[language=Python]
class BahdanauAttention(layers.Layer):
    def __init__(self, units):
        super(BahdanauAttention, self).__init__()
        self.W1 = layers.Dense(units)
        self.W2 = layers.Dense(units)
        self.V = layers.Dense(1)

    def call(self, query, values):
        query_with_time_axis = tf.expand_dims(query, 1)
        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))
        attention_weights = tf.nn.softmax(score, axis=1)
        context_vector = attention_weights * values
        context_vector = tf.reduce_sum(context_vector, axis=1)
        return context_vector, attention_weights

attention_layer = BahdanauAttention(10)
attention_result, attention_weights = attention_layer(encoder_state, encoder_output)

print("Bahdanau Attention result shape:", attention_result.shape)
print("Bahdanau Attention weights shape:", attention_weights.shape)
\end{lstlisting}

\newpage

\subsection{Self-Attention}
Transformer modellerinin temel bileşenidir. Bir metin içindeki tüm öğelerin birbirine dikkat etmesini sağlar. Paralel hesaplama imkanı sağlar. Girdi dizisi sorgu (query), key(anahtar) ve değer vektörleri (value) olmask üzere üç farklı vektöre dönüştürülür. Sorgu ve anahtar vektörlerinin nokta çarpımı (dot product) ile skorlar hesaplanır ve ölçeklendirilir, elde edilen değerler softmax ile normalize edilir. Ağırlık ortalama alınarak yeni bir vektör elde edilir.

\newpage

\subsection{Multi-Head Attention}
Birden fazla self-attention mekanizmasını paralel olarak kullanır ve çıktıları birleştirir. Girdi vektörleri, farklı ağırlık matriksleri ile farklı başlıklara (heads) bölünür. Her başlık için bağımsız self-attention uygulanır. Tüm başlıklardan elde edilen çıktılar birleştirilir ve son bir ağırlık matrisi ile yeniden dönüştürülür.

\newpage

\subsection{Hierarchical Attention}
Çok katmanlı yapıların farklı seviyelerinde dikkat mekanizmalarını kullanır. Genellikle doküman seviyesi ve cümle seviyesi gibi hiyerarşik yapılarda kullanılır.

\begin{itemize}
	\item \textbf{Cümle Seviyesi}: İlk olarak cümle içindeki kelimeler arasında dikkat hesaplanır ve cümle temsilleri oluşturulur.
	\item \textbf{Doküman Seviyes}: Ardından, cümle temsilleri arasında dikkat hesaplanarak doküman temsili elde edilir.
\end{itemize}

\subsubsection{Python Kodu}

TensorFlow kütüphanesi ile "Hierarchical Attention Network" implementasyonu yapılmıştır.

\begin{lstlisting}[language=Python]
class AttentionWithContext(layers.Layer):
    def __init__(self,
                 W_regularizer=None, u_regularizer=None, b_regularizer=None,
                 W_constraint=None, u_constraint=None, b_constraint=None,
                 bias=True, **kwargs):
        self.init = initializers.get('glorot_uniform')

        self.W_regularizer = regularizers.get(W_regularizer)
        self.u_regularizer = regularizers.get(u_regularizer)
        self.b_regularizer = regularizers.get(b_regularizer)

        self.W_constraint = constraints.get(W_constraint)
        self.u_constraint = constraints.get(u_constraint)
        self.b_constraint = constraints.get(b_constraint)

        self.bias = bias
        super(AttentionWithContext, self).__init__(**kwargs)

    def build(self, input_shape):
        assert len(input_shape) == 3

        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1]),
                                 initializer=self.init,
                                 name='{}_W'.format(self.name),
                                 regularizer=self.W_regularizer,
                                 constraint=self.W_constraint)
        if self.bias:
            self.b = self.add_weight(shape=(input_shape[-1],),
                                     initializer='zero',
                                     name='{}_b'.format(self.name),
                                     regularizer=self.b_regularizer,
                                     constraint=self.b_constraint)

        self.u = self.add_weight(shape=(input_shape[-1], 1),
                                 initializer=self.init,
                                 name='{}_u'.format(self.name),
                                 regularizer=self.u_regularizer,
                                 constraint=self.u_constraint)

        super(AttentionWithContext, self).build(input_shape)

    def call(self, x, mask=None):
        uit = K.dot(x, self.W)

        if self.bias:
            uit += self.b
            
        uit = K.tanh(uit)
        ait = K.squeeze(K.dot(uit, self.u), -1)
        a = K.exp(ait)
        if mask is not None:
            a *= K.cast(mask, K.floatx())

        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())
        a = K.expand_dims(a)
        weighted_input = x * a
        return K.sum(weighted_input, axis=1)

    def compute_output_shape(self, input_shape):
        return input_shape[0], input_shape[-1]

word_input = layers.Input(shape=(max_sentence_len,), dtype='float32')
x = layers.Embedding(len(tokenizer.word_index) + 1, 
                     embed_size, 
                     weights=[embedding_matrix], 
                     input_length=max_sentence_len, 
                     trainable=False)(word_input)
x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, kernel_regularizer="l2"))(x)
x = layers.TimeDistributed(layers.Dense(256, kernel_regularizer="l2"))(x)
x = AttentionWithContext()(x)
word_encoder = models.Model(word_input, x)

sentence_input = layers.Input(shape=(max_sentence_num, max_sentence_len), dtype='float32')
x = layers.TimeDistributed(word_encoder)(sentence_input)
x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, kernel_regularizer="l2"))(x)
x = layers.TimeDistributed(layers.Dense(256, kernel_regularizer="l2"))(x)
x = layers.Dropout(0.5)(AttentionWithContext()(x))
output_layer = layers.Dense(authors.nunique(), activation='softmax')(x)

model = models.Model(sentence_input, output_layer)
model.compile(loss='categorical_crossentropy', 
              optimizer='adam', 
              metrics=['accuracy'])
\end{lstlisting}

\newpage

\subsection{Monotonic Attention}
Sıralı veriler ile çalışırken dikkatin yalnızca belirli bir yönü takip etmesini sağlar. Bu, dikkatin sıralı bir şekilde ilerlemesini zorunlu kılar. Dikkat ağırlıkları, sıralı bir şekilde ilerleyerek hesaplanır. Bir konumdan bir sonraki geçişler sıralı olur. Bağlam vektörü, diğerlerine benzer şekilde hesaplanır ancak dikkatin sıralılığı korunur.

\newpage

\subsection{Luong Monotic Attention}
Genellikle konuşma tanıma problemlerinde kullanılır. Luong Attention mekanizmasının sıralı veriler ile çalışacak şekilde düzenlenmiş halidir. Dikkat mekanizması zaman ilerledikçe sıralı ve monoton bir şekilde ilerler.

\newpage

\subsection{Bahdanau Monotic Attention}
Bahdanau Attention mekanizmasının sıralı veriler ile çalışacak şekilde düzenlenmiş halidir. Dikkat mekanizması zaman ilerledikçe sıralı ve monoton bir şekilde ilerler.

\newpage

\subsection{Dot Product Attention}

Dot Product Attention, verilen bir sorgu (query) vektörünün, bir dizi anahtar (key) vektörüyle olan benzerliğini hesaplayarak, her bir anahtarın karşılık geldiği değer (value) vektörüne ne kadar dikkat edilmesi gerektiğini belirler. Benzerlik, sorgu ve anahtar vektörlerinin dot product (iç çarpım) işlemi ile hesaplanır.

\begin{itemize}
    \item \textbf{Query (Sorgu)}: Bir odaklanılacak öğeyi temsil eder.
    \item \textbf{Key (Anahtar)}: Girişin diğer öğeleriyle sorguyu eşleştirmek için kullanılır.
    \item \textbf{Value (Değer)}: Her anahtar için bir temsilci değer vektörüdür.
\end{itemize}

\subsubsection{Çalışma Adımları}

Her bir giriş öğesi için sorgu, anahtar ve değer vektörleri elde edilir. Bu vektörler, genellikle öğrenilebilir parametrelerle tanımlanır ve matris çarpımlarıyla elde edilir.

Her bir sorgu vektörü ile her bir anahtar vektörü arasında dot product (iç çarpım) hesaplanır. Bu, sorgu ve anahtar vektörlerinin boyutlarına bağlı olarak bir skalar değer üretir. Dot product, sorgunun anahtarla ne kadar benzer olduğunu ölçer.

\[ \text{attention\_score}_{ij} = \text{query}_i \cdot \text{key}_{j}^{T} \]

Hesaplanan dot product skorları, softmax fonksiyonu kullanılarak normalize edilir. Softmax, bu skorları olasılık dağılımına dönüştürür, yani tüm skorların toplamı 1 olur. Böylece hangi anahtarın sorguyla en alakalı olduğunu belirlemek kolaylaşır.

\[ \alpha_{ij} = \frac{\text{exp}(\text{attention\_score}_{ij})}{\sum_k \text{exp}(\text{attention\_score}_{ik})} \]

Son adımda, her bir değer vektörü, softmax sonucu elde edilen olasılıklarla (yani ağırlıklarla) çarpılır ve toplanır. Bu, her sorgu için ilgili değerlere odaklanan bir çıkış vektörü oluşturur.

\[ \text{output}_i = \sum_j \alpha_{ij} \times value_j \]

\newpage

\subsection{Causal Attention}

Causal Attention, sıralı veriler üzerinde çalışırken yalnızca önceki öğelere bakmayı garanti eden bir dikkat mekanizmasıdır. Yani, bir kelime veya veri parçası analiz edilirken, yalnızca kendisinden önce gelen verilere dikkat edebilir; bu, gelecekteki verileri hesaba katmaz. Bu, özellikle dil modellemesi ve zaman serisi tahmini gibi gelecekteki bilgiyi tahmin etmemiz gereken durumlar için önemlidir. Bu mekanizma, nedensellik ilkesini koruyarak, gelecekteki bilgiyi kullanmanın yaratabileceği yanlış tahminleri engeller ve modeli daha gerçekçi bir şekilde öğrenmeye yönlendirir. 

Causal Attention, standart attention mekanizmasındaki sorgu (query), anahtar (key) ve değer (value) vektörleri üzerinde çalışır. Ancak, bu mekanizmada yalnızca geçmiş vektörlere dikkat edilebilir. Dolayısıyla, gelecekteki bilgiyi dikkate alan bir yapı engellenir. 

Causal Attention'da gelecekteki bilgiyi engellemek için bir mask (maskeleme) işlemi yapılır. Bu maskeleme, gelecekteki veriye erişimi sınırlamak için matris üzerinde uygulanır. Özellikle dil modellerinde, kelime sırasına göre bir maskeleme yapılır.

\newpage

\subsection{Reformer Efficient Attention}

Reformer Efficient Attention, 2020 yılında Google Research tarafından önerilen Reformer adlı bir modelde kullanılan verimli bir dikkat mekanizmasıdır. Reformer Efficient Attention, özellikle uzun dizileri işlerken gereksiz hesaplamaları ve bellek kullanımını azaltarak, Transformers'ların sınırlamalarını aşmayı sağlar. Geleneksel dikkat mekanizmaları $O(n^2)$ (dizinin uzunluğuna göre karekök orantılı) bir hesaplama maliyeti taşır. Yani dizinin uzunluğu arttıkça işlem maliyeti katlanarak artar. Reformer bu maliyeti logaritmik seviyelere indirir, bu da uzun diziler üzerinde çalışırken büyük bir hızlanma sağlar.

Reformer Efficient Attention’ın temel çalışma prensibi, dikkat hesaplamalarını optimize etmek için Locality-Sensitive Hashing (LSH) ve revizyonlu ağlar (reversible networks) gibi yenilikçi teknikler kullanmasıdır.

\subsubsection{Locality-Sensitive Hashing (LSH)}

Geleneksel dikkat mekanizmasında, her bir öğe (kelime, karakter vb.) diğer tüm öğelerle etkileşime girer. Bu, büyük veri kümeleri için yüksek hesaplama ve bellek maliyetine neden olur. Locality-Sensitive Hashing (LSH), dikkat hesaplamasını verimli hale getirir. LSH, öğeleri benzerliğe göre gruplandırarak, her bir öğenin yalnızca kendisine benzeyen diğer öğelere dikkat etmesini sağlar. Bu şekilde, tüm öğeler arasında hesaplama yapmak yerine, yalnızca benzer olan öğeler arasında dikkati hesaplar. Giriş öğeleri hash fonksiyonlarıyla daha küçük gruplara ayrılır. Bu gruplar içindeki öğeler, birbirine daha yakın ve daha benzer olduğu kabul edilir. Böylece, her öğe sadece kendi grubundaki öğelere dikkat eder, bu da hesaplamayı hızlandırır. Hash'leme sayesinde tüm öğeler arasında dikkat hesaplaması yapılmaz, sadece benzer olanların dikkat hesaplaması yapılır. Bu, geleneksel attention'daki $O(n^2)$ maliyeti, $O(n \text{log} n)$'ye düşürür. 

\subsubsection{Reversible Networks (Geri Dönüşümlü Ağlar)}

Reformer, belleği verimli kullanmak için reversible networks kullanır. Geleneksel Transformer'larda her bir katman, önceki katmanın çıktısını depolar ve her adımda bu çıktıyı kullanır. Bu, bellekte yüksek bir yük oluşturur. Reformer'da ise, geri dönüşümlü ağlar sayesinde önceki katmanların çıktıları bellekte depolanmaz, çünkü her adımda bu çıktılar yeniden hesaplanabilir. Bu, belleği büyük ölçüde azaltır. Modeldeki tüm ara hesaplamaları bellekte saklamak yerine, sadece en gerekli bilgiler saklanır ve gerektiğinde hesaplamalar yeniden yapılır.

\subsubsection{Chunking (Sıralı Karıştırma)}

Reformer, uzun dizileri işlemeyi kolaylaştırmak için veriyi küçük parçalara ayırır ve bu parçalarda dikkat hesaplaması yapar. Bu teknik sayesinde, hesaplama işlemi küçük diziler üzerinde yürütülerek maliyet düşürülür.

\subsubsection{Çalışma Adımları}

\begin{enumerate}
    \item Uzun bir dizi girdi alındığında, Reformer bu girdiyi daha küçük parçalar (chunks) halinde böler. Bu, uzun diziler üzerinde çalışırken işlem maliyetini düşürmeye yardımcı olur.
    \item Her bir parça için, LSH kullanılarak dikkat hesaplaması yapılacak öğeler seçilir. Öğeler, benzer olanlar arasında gruplanır ve dikkat hesaplamaları yalnızca benzer öğeler arasında gerçekleştirilir. Bu, dikkat hesaplamasını hızlandırır.
    \item LSH ile benzer gruplara ayrılan öğeler arasında, dikkat (attention) skorları hesaplanır. Her bir öğenin sorgu (query), anahtar (key) ve değer (value) vektörleri arasında dot product hesaplanarak dikkat skorları elde edilir.
    \item Modelin farklı katmanlarında kullanılan ara hesaplamalar, geri dönüşümlü ağlar sayesinde yeniden hesaplanır ve bellek kullanımı optimize edilir. Bu, özellikle derin modellerde büyük bellek tasarrufu sağlar.
    \item Dikkat skorları hesaplandıktan sonra, bu skorlar ağırlıklandırılarak çıkış vektörleri hesaplanır ve bir sonraki katmana aktarılır.
\end{enumerate}

\newpage

\subsection{Iterative Attention}

Standart dikkat mekanizmalarının geliştirilmiş bir formu olarak, modelin dikkatle odaklandığı bilgilere sürekli geri dönerek kararlarını daha doğru bir şekilde vermesini sağlar. Iterative Attention mekanizması, modelin dikkatini iteratif bir şekilde farklı bilgi kaynaklarına dağıtarak zamanla daha hassas sonuçlar elde etmesini sağlar. Her adımda dikkat mekanizması bir geri bildirim döngüsüne girer ve önceki iterasyonlarda yapılan hata veya eksik bilgilere göre kendini günceller.

\newpage

\subsection{Aggregated Attention}

Aggregated Attention, standart dikkat mekanizmasındaki çoklu dikkat başlıklarını (multi-head attention) bir araya getirip daha fazla bilgi toplamak amacıyla geliştirilmiştir. Her bir dikkat başlığı, modelin farklı özelliklere odaklanmasını sağlar ve bu başlıkların birleştirilmesi (aggregation), daha zengin ve kapsamlı temsiller oluşturur. Geleneksel çoklu başlıklı dikkat mekanizmasında, her dikkat başlığı veriyi farklı açılardan ele alır. Ancak, bu başlıklar ayrı ayrı değerlendirilir ve her biri kendi bilgi kümesine göre modelleme yapar. Aggregated Attention ise bu başlıkların ürettiği dikkat çıktısını birleştirir ve daha geniş bir bağlam sağlar.

Dikkat mekanizmasındaki çoklu başlıklar, giriş verisini farklı alt uzaylara projekte ederek farklı ilişkileri öğrenir. Her başlık, belirli bir özellik kümesine odaklanır. Çoklu başlıkların ürettiği dikkat değerleri, toplanarak veya birleştirilerek daha kapsamlı bir bağlam elde edilir. Bu adım, aynı veri üzerinde birden fazla perspektiften çıkarım yapmaya olanak tanır. Birleştirilmiş dikkat çıktısı, modelin daha geniş bir bağlamsal bilgiye sahip olmasını sağlar.

\newpage

\subsection{Cross Attention}

Transformers mimarisinde kullanılan "Self Attention" kavramının bir uzantısıdır. Cross Attention, iki farklı veri kaynağı veya özellik uzayı arasında ilişki kurmaya yarar. Bu, bir veri kümesindeki bilgilerin diğer bir veri kümesiyle nasıl etkileşim kurduğunu anlamayı sağlar. Bu mekanizma, çift taraflı bilgi alışverişi gerektiren görevlerde oldukça etkilidir. Self Attention, bir veri kümesindeki öğelerin kendi aralarındaki ilişkilerini öğrenirken, Cross Attention ise bir veri kümesinin öğelerini başka bir veri kümesinin öğeleriyle bağdaştırır.

Cross Attention, üç temel bileşen üzerinden çalışır: Query (Sorgu), Key (Anahtar) ve Value (Değer). İki veri kümesi arasında bu işlemler yapılır.

\begin{itemize}
    \item \textbf{Query (Sorgu)}: İlk veri kümesinden sorgular çıkarılır. Bu sorgular, ikinci veri kümesindeki öğelerle etkileşim kurmak için kullanılır.
    \item \textbf{Key (Anahtar)}: İkinci veri kümesinden anahtarlar çıkarılır. Bu anahtarlar, hangi öğelerin önemli olduğunu belirlemeye yardımcı olur.
    \item \textbf{Value (Değer)}: Anahtarlar aracılığıyla seçilen öğelerin değerleri alınır ve sorgularla ilişkilendirilir.
\end{itemize}

\subsubsection{Çalışma Adımları}

\begin{enumerate}
    \item İlk veri kümesinden Query (sorgular), ikinci veri kümesinden Key (anahtarlar) ve Value (değerler) çıkarılır. Bu adım, farklı modaliteler arasındaki ilişkilerin kurulacağı temeli oluşturur.
    \item Query ve Key arasında bir skor matrisi hesaplanır. Bu matris, hangi Query öğesinin hangi Key öğesiyle en ilişkili olduğunu belirlemek için kullanılır. Skorlar, genellikle Query ve Key vektörlerinin nokta çarpımı (dot product) ile hesaplanır.
    \item Hesaplanan skorlar, softmax fonksiyonundan geçirilir. Bu adım, ilişkilerin normalize edilmesini sağlar ve hangi key öğesinin daha önemli olduğunu belirler.
    \item Softmax ile normalize edilen skorlar, ilgili Value vektörlerine uygulanır. Bu sayede en önemli Key-Value çiftleri daha fazla dikkate alınır.
    \item Ağırlıklandırılmış değerler toplanarak Cross Attention mekanizmasının nihai çıktısı elde edilir.
\end{enumerate}

\newpage